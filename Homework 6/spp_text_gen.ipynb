{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spp_text_gen.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAZR6a/i+eKf2FSfXufDPY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%206/spp_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acFBnEnpbuun",
        "colab_type": "code",
        "outputId": "9189ed23-2479-41bf-acde-18fd9aa6aa48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a62WDbTn7EhM",
        "colab_type": "code",
        "outputId": "9c9561ef-1ca6-4fd7-a2d8-9233b588a4ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "root_directory = 'drive/Shared drives/cis530_hw6/data'\n",
        "!ls 'drive/Shared drives/cis530_hw6/data'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all-abcs.txt  eng-fra.txt  jane_austen.txt  names  trump-speeches-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kAo_x9ybkRD",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7qVxd6YbyDJ",
        "colab_type": "code",
        "outputId": "9f142742-56b0-46ba-8db1-188dd42fd898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls 'drive/Shared drives/cis530_hw6/data/'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all-abcs.txt  eng-fra.txt  jane_austen.txt  names  trump-speeches-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKdbHV1TgbGQ",
        "colab_type": "code",
        "outputId": "1d76a0d9-c42e-480b-fc90-bacfa2f0efae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "root_directory = 'drive/Shared drives/cis530_hw6/data/'\n",
        "text_directory = 'jane_austen.txt'\n",
        "\n",
        "file = open(root_directory + text_directory, encoding = 'UTF-8', errors = 'ignore').read()\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 4373592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1YTZ6e_0qp5",
        "colab_type": "code",
        "outputId": "18f05345-66f0-4a08-b4a8-83036c92779a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rcumstances\n",
            "under which she felt them. Nay, had she been without his arm, she would\n",
            "soon have known that she needed it, for she wanted strength for a two\n",
            "hours' saunter of this kind, coming, as it gene\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwEKzEse0weu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "        \n",
        "    self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "    self.decoder = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "  def forward(self, input, hidden):\n",
        "    input = self.encoder(input.view(1, -1))\n",
        "    output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
        "    output = self.decoder(output.view(1, -1))\n",
        "    return output, hidden\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEGPLSJ60_XS",
        "colab_type": "code",
        "outputId": "f68f83f8-50bc-4fce-e9f4-f9670eda067e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "    tensor[c] = all_characters.index(string[c])\n",
        "  return Variable(tensor)\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_1wvvA11Js9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_training_set():    \n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b87_Dz81N_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  hidden = decoder.init_hidden()\n",
        "  prime_input = char_tensor(prime_str)\n",
        "  predicted = prime_str\n",
        "  \n",
        "  # Use priming string to \"build up\" hidden state\n",
        "  for p in range(len(prime_str) - 1):\n",
        "    _, hidden = decoder(prime_input[p], hidden)\n",
        "  inp = prime_input[-1]\n",
        "  \n",
        "  for p in range(predict_len):\n",
        "    output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "    # Sample from the network as a multinomial distribution\n",
        "    output_dist = output.data.view(-1).div(temperature).exp()\n",
        "    top_i = torch.multinomial(output_dist, 1)[0]\n",
        "    \n",
        "    # Add predicted character to string and use as next input\n",
        "    predicted_char = all_characters[top_i]\n",
        "    predicted += predicted_char\n",
        "    inp = char_tensor(predicted_char)\n",
        "\n",
        "  return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBQ_m6x61aO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time, math\n",
        "\n",
        "def time_since(since):\n",
        "  s = time.time() - since\n",
        "  m = math.floor(s / 60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLLhqbK01hQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(inp, target):\n",
        "  hidden = decoder.init_hidden()\n",
        "  decoder.zero_grad()\n",
        "  loss = 0\n",
        "\n",
        "  for c in range(chunk_len):\n",
        "      output, hidden = decoder(inp[c], hidden)\n",
        "      loss += criterion(output, target[[c]])\n",
        "\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.data / chunk_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN7UBSKG1j6H",
        "colab_type": "code",
        "outputId": "221d7294-0b66-4802-bcf2-aaf348723c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss = train(*random_training_set())       \n",
        "  loss_avg += loss\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0m 18s (100 5%) 2.2677]\n",
            "Whe of anh oand ing ant wen at seivecet edes neme hopplen in, been\n",
            "Ne and dimeette it anst thed wat o  \n",
            "\n",
            "[0m 38s (200 10%) 2.2846]\n",
            "Whe rece in the renesion an tho wlepronting to had veamppr.\"\n",
            "\n",
            "\"I githat han the youd a sucher dis\" who \n",
            "\n",
            "[0m 56s (300 15%) 1.9988]\n",
            "Wh-!\"\n",
            "\"II with our and the\n",
            "saik a doevelry on sure have and be pexpooent te care the\n",
            "agist then in I d \n",
            "\n",
            "[1m 14s (400 20%) 1.8200]\n",
            "Whithe invenied accelfulsoned theled in tilmittiot whure would have sis perfore\n",
            "to count\n",
            "for a colless \n",
            "\n",
            "[1m 32s (500 25%) 1.9626]\n",
            "Whilly wideass of in of monlauge othing. She mowanter in alfted the seate. So more of the mright af an \n",
            "\n",
            "[1m 50s (600 30%) 1.8790]\n",
            "Whe Sering be Chave with hard your piturition thourise to olf ill a\n",
            "mainer. Could Mrs. Norner, had be  \n",
            "\n",
            "[2m 8s (700 35%) 1.7199]\n",
            "Wherful Craabfan anys! My fell to him\n",
            "happuring of being toow her som, was you so do, the pursent it a \n",
            "\n",
            "[2m 26s (800 40%) 2.0343]\n",
            "Wht, must surmed the to the of to was enou propriot not laterse to ready sact corle\n",
            "at Mr-Dary! Chate  \n",
            "\n",
            "[2m 44s (900 45%) 1.6718]\n",
            "Whould wain to adde was of the spocher the for her\n",
            "conferied the was of Mrs. Hard have onger\n",
            "was at th \n",
            "\n",
            "[3m 2s (1000 50%) 1.8386]\n",
            "Whould to had comporre disgrear their porranters smate more in offere Elizabe the more, and this from  \n",
            "\n",
            "[3m 20s (1100 55%) 1.6211]\n",
            "Who so uncount Lay Elinore\n",
            "we gave nexer\n",
            "so,\n",
            "and wis the jound now to gade of the sady of gount some w \n",
            "\n",
            "[3m 38s (1200 60%) 2.0642]\n",
            "Whlued the was perted for and his to there.                                                            \n",
            "\n",
            "[3m 55s (1300 65%) 1.8632]\n",
            "Whould not that enougk, be disiclatiniin migre thighter her from gorly she a mend the is agreeation a  \n",
            "\n",
            "[4m 13s (1400 70%) 1.6514]\n",
            "Whink of the gilled be well she word that your fume the selsess of be presity than sain\n",
            "the par; and F \n",
            "\n",
            "[4m 30s (1500 75%) 1.5242]\n",
            "Whosson spires, at she with witt wold, and of that suppow ond heary.  At the cancultonge to which anxa \n",
            "\n",
            "[4m 48s (1600 80%) 1.9103]\n",
            "Whis aswail she wins not her be will been to on the condented of what for the in his his she had not s \n",
            "\n",
            "[5m 6s (1700 85%) 1.7706]\n",
            "Whough the succutant, though becempsed to my it drivied to mith could hers.\n",
            "\n",
            "\n",
            "\"Yes sistion, and trour  \n",
            "\n",
            "[5m 23s (1800 90%) 1.6992]\n",
            "Wh, assider--reat to have such of when simane, and been a reeine of his do nothing exever then and inl \n",
            "\n",
            "[5m 41s (1900 95%) 1.9985]\n",
            "Whbou that your spection with the a plading the gose and as neenes to everymence to had beyed, he as a \n",
            "\n",
            "[5m 58s (2000 100%) 1.9159]\n",
            "When now soomention of the ober, fon bouse which not the be\n",
            "the plassed uund convered.\n",
            "\n",
            "Harrie, and I  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P2S5fjNmTcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def perplexity(input_text):\n",
        "  softmax = nn.Softmax()\n",
        "  full_input_tensor = char_tensor(input_text)\n",
        "  hidden = decoder.init_hidden()\n",
        "  prob = 0\n",
        "  for i in range(len(input_text)-1):\n",
        "      output, hidden = decoder(full_input_tensor[i], hidden)\n",
        "      output = softmax(output)\n",
        "      next_char = input_text[i+1]\n",
        "      if next_char in all_characters:\n",
        "          next_char = all_characters.index(next_char)\n",
        "      else:\n",
        "          next_char = 84\n",
        "      prob_next = math.log(output[0][next_char])\n",
        "      prob = prob + prob_next\n",
        "  prob = -1/(len(input_text)-1)*prob\n",
        "  prob = math.exp(prob)\n",
        "  return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR-QF-GfnkDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "9eeb3fc3-7788-492c-d477-cb26ebd8b3ac"
      },
      "source": [
        "text_directory = 'jane_austen.txt'\n",
        "\n",
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "random_text = random_chunk()\n",
        "print(perplexity(random_text))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.095214842603305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}