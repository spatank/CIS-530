{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%207/homework_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwJLStdTrMek",
        "colab_type": "code",
        "outputId": "d4a8ae87-2c90-4c0f-8cc5-66eb75e0a8fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWpGgT7v0-2P",
        "colab_type": "code",
        "outputId": "adda63e1-8948-4e5b-ce6a-e36c22805af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# from nltk.corpus import conll2002\n",
        "import nltk \n",
        "nltk.download('conll2002')\n",
        "from nltk.corpus import conll2002\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import Perceptron, RidgeClassifier, PassiveAggressiveClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pickle\n",
        "\n",
        "# Assignment 7: NER\n",
        "# Rebecca Iglesias-Flores and Shubhankar Patankar\n",
        "# This is just to help you get going. Feel free to\n",
        "# add to or modify any part of it."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2002 is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wLNxPvR06Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hasDot(word):\n",
        "  return int('.' in word)\n",
        "\n",
        "def hasApost(word):\n",
        "  return int('\\'' in word)\n",
        "\n",
        "def hasHyph(word):\n",
        "  return int('-' in word)\n",
        "\n",
        "def hasNC(pos):\n",
        "  return int('NC' in pos)\n",
        "\n",
        "def hasAQ(pos):\n",
        "  return int('AQ' in pos)\n",
        "\n",
        "def isCap(word):\n",
        "    ## if first letter of the word is capitalized\n",
        "  return int(word[0].isupper()) \n",
        "\n",
        "def hasCap(word):\n",
        "    ## if any letter of the word is capitalized or not\n",
        "  return int(word.islower())\n",
        "\n",
        "accents = 'ÂÃÄÀÁÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖØÙÚÛÜÝÞßàáâãäåæçèéêëìíîïðñòóôõöøùúûüýþÿ'\n",
        "\n",
        "def hasAcc(word):\n",
        "  for char in word:\n",
        "    if char in accents:\n",
        "      return int(True)\n",
        "  return int(False)\n",
        "\n",
        "def hasDig(word):\n",
        "  ## assigns a 1 if there's a digit in the word\n",
        "  return int(word.isalpha())\n",
        "\n",
        "def prefix(word):\n",
        "  return word[:4]\n",
        "\n",
        "def suffix(word):\n",
        "  return word[-4:]\n",
        "\n",
        "def wordshape(word):\n",
        "    import re\n",
        "    t1 = re.sub('[A-Z]', 'X', word)\n",
        "    t2 = re.sub('[a-z]', 'x', t1)\n",
        "    return re.sub('[0-9]', 'd', t2)\n",
        "\n",
        "def getfeats(word, pos_tag, o):\n",
        "  \"\"\" This takes the word in question and\n",
        "  the offset with respect to the instance\n",
        "  word \"\"\"\n",
        "  o = str(o)\n",
        "  features = [\n",
        "              (o + 'word', word),\n",
        "              (o + 'shape', wordshape(word)),\n",
        "              # TODO: add more features here.\n",
        "              # (o + 'hasDot', hasDot(word)),\n",
        "              (o + 'hasApost', hasApost(word)),\n",
        "              (o + 'hasHyph', hasHyph(word)),\n",
        "              (o + 'hasNC', hasNC(pos_tag)),\n",
        "              (o + 'hasAQ', hasAQ(pos_tag)),\n",
        "              (o + 'isCap', isCap(word)),\n",
        "              (o + 'hasCap', hasCap(word)),\n",
        "              (o + 'hasAcc', hasAcc(word)),\n",
        "              # (o + 'hasDig', hasDig(word)),\n",
        "              (o + 'prefix', prefix(word)),\n",
        "              (o + 'suffix', suffix(word))\n",
        "              ]\n",
        "  return features\n",
        "    \n",
        "def word2features(sent, i):\n",
        "  \"\"\" The function generates all features\n",
        "  for the word at position i in the\n",
        "  sentence.\"\"\"\n",
        "  features = []\n",
        "  # the window around the token\n",
        "  for o in [-3,-2,-1,0,1,2,3]:\n",
        "    if i+o >= 0 and i+o < len(sent):\n",
        "      word = sent[i+o][0]\n",
        "      pos_tag = sent[i+o][1]\n",
        "      featlist = getfeats(word, pos_tag, o)\n",
        "      features.extend(featlist)\n",
        "  \n",
        "  return dict(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulbN7T0LD5Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sents = list(conll2002.iob_sents('esp.train'))\n",
        "dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
        "test_sents = list(conll2002.iob_sents('esp.testb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8K_1c8yEU5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c64a8dc6-6518-455d-a5c4-c6afdd46e6c7"
      },
      "source": [
        "dev_sents[108]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('(', 'Fpa', 'O'),\n",
              " ('URGENCIAS', 'NC', 'O'),\n",
              " ('061', 'Z', 'O'),\n",
              " (')', 'Fpt', 'O'),\n",
              " ('.', 'Fp', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPe5FnsQC9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_feats = []\n",
        "# train_labels = []\n",
        "\n",
        "# for sent in train_sents:\n",
        "#   for i in range(len(sent)):\n",
        "#     feats = word2features(sent, i)\n",
        "#     train_feats.append(feats)\n",
        "#     train_labels.append(sent[i][-1])\n",
        "\n",
        "# vectorizer = DictVectorizer()\n",
        "# X_train = vectorizer.fit_transform(train_feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeDSiRxj6rYk",
        "colab_type": "code",
        "outputId": "3b0b388b-1d82-4e73-d282-92a12179f8f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  train_sents = list(conll2002.iob_sents('esp.train'))\n",
        "  train_feats = []\n",
        "  train_labels = []\n",
        "  for sent in train_sents:\n",
        "    for i in range(len(sent)):\n",
        "      feats = word2features(sent,i)\n",
        "      train_feats.append(feats)\n",
        "      train_labels.append(sent[i][-1])\n",
        "\n",
        "  vectorizer = DictVectorizer()\n",
        "  X_train = vectorizer.fit_transform(train_feats)\n",
        "\n",
        "  # model = Perceptron(verbose = 1, max_iter = 2000)\n",
        "  # model = RidgeClassifier()\n",
        "  model = PassiveAggressiveClassifier(verbose = 1, loss = 'squared_hinge')\n",
        "  # model = SGDClassifier(verbose = 1)\n",
        "  model.fit(X_train, train_labels)\n",
        "  pickle.dump(model, open('drive/My Drive/CIS-530/Homework 7/Results/model', 'wb'))\n",
        "\n",
        "  # TRAINING SET\n",
        "  y_pred_train = model.predict(X_train)  \n",
        "  j = 0\n",
        "  print(\"Writing to train_results.txt\")\n",
        "  # format is: word gold pred\n",
        "  outfile_path = 'drive/My Drive/CIS-530/Homework 7/Results/train_results.txt' \n",
        "  with open(outfile_path, \"w\") as out:\n",
        "    for sent in train_sents: \n",
        "      for i in range(len(sent)):\n",
        "        word = sent[i][0]\n",
        "        gold = sent[i][-1]\n",
        "        pred = y_pred_train[j]\n",
        "        j += 1\n",
        "        out.write(\"{}\\t{}\\t{}\\n\".format(word, gold, pred))\n",
        "    out.write(\"\\n\")\n",
        "\n",
        "  # DEVELOPMENT SET\n",
        "  dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
        "  dev_feats = []\n",
        "  dev_labels = []\n",
        "  for sent in dev_sents:\n",
        "    for i in range(len(sent)):\n",
        "      feats = word2features(sent,i)\n",
        "      dev_feats.append(feats)\n",
        "      dev_labels.append(sent[i][-1])\n",
        "  X_dev = vectorizer.transform(dev_feats)\n",
        "  y_pred_dev = model.predict(X_dev)\n",
        "  j = 0\n",
        "  print(\"Writing to dev_results.txt\")\n",
        "  # format is: word gold pred\n",
        "  outfile_path = 'drive/My Drive/CIS-530/Homework 7/Results/dev_results.txt'\n",
        "  with open(outfile_path, \"w\") as out:\n",
        "    for sent in dev_sents: \n",
        "      for i in range(len(sent)):\n",
        "        word = sent[i][0]\n",
        "        gold = sent[i][-1]\n",
        "        pred = y_pred_dev[j]\n",
        "        j += 1\n",
        "        out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "    out.write(\"\\n\")\n",
        "\n",
        "  # TEST SET\n",
        "  test_sents = list(conll2002.iob_sents('esp.testb'))\n",
        "  test_feats = []\n",
        "  test_labels = []\n",
        "  for sent in test_sents:\n",
        "    for i in range(len(sent)):\n",
        "      feats = word2features(sent,i)\n",
        "      test_feats.append(feats)\n",
        "      test_labels.append(sent[i][-1])\n",
        "  X_test = vectorizer.transform(test_feats)\n",
        "  y_pred_test = model.predict(X_test)\n",
        "  j = 0\n",
        "  print(\"Writing to test_results.txt\")\n",
        "  # format is: word gold pred\n",
        "  outfile_path = 'drive/My Drive/CIS-530/Homework 7/Results/test_results.txt'\n",
        "  with open(outfile_path, \"w\") as out:\n",
        "    for sent in test_sents: \n",
        "      for i in range(len(sent)):\n",
        "        word = sent[i][0]\n",
        "        gold = sent[i][-1]\n",
        "        pred = y_pred_test[j]\n",
        "        j += 1\n",
        "        out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "    out.write(\"\\n\")\n",
        "\n",
        "  print(\"Now run: python conlleval.py test_results.txt\")"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 13.37, NNZs: 61142, Bias: -0.083439, T: 264715, Avg. loss: 0.032485\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.53, NNZs: 69338, Bias: -0.097949, T: 529430, Avg. loss: 0.018526\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.37, NNZs: 72538, Bias: -0.103246, T: 794145, Avg. loss: 0.014014\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 22.58, NNZs: 74295, Bias: -0.107065, T: 1058860, Avg. loss: 0.011057\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 24.31, NNZs: 75360, Bias: -0.111805, T: 1323575, Avg. loss: 0.009125\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 25.77, NNZs: 75922, Bias: -0.113951, T: 1588290, Avg. loss: 0.007848\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 27.00, NNZs: 76250, Bias: -0.115219, T: 1853005, Avg. loss: 0.006821\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 28.04, NNZs: 76498, Bias: -0.116651, T: 2117720, Avg. loss: 0.005911\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 28.97, NNZs: 76704, Bias: -0.118333, T: 2382435, Avg. loss: 0.005368\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 29.78, NNZs: 76884, Bias: -0.120627, T: 2647150, Avg. loss: 0.004788\n",
            "Total training time: 1.55 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 30.52, NNZs: 77040, Bias: -0.121897, T: 2911865, Avg. loss: 0.004461\n",
            "Total training time: 1.69 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 31.19, NNZs: 77144, Bias: -0.123013, T: 3176580, Avg. loss: 0.004122\n",
            "Total training time: 1.85 seconds.\n",
            "Convergence after 12 epochs took 1.85 seconds\n",
            "-- Epoch 1\n",
            "Norm: 11.23, NNZs: 58853, Bias: -0.067224, T: 264715, Avg. loss: 0.023155\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 14.88, NNZs: 68458, Bias: -0.075597, T: 529430, Avg. loss: 0.013148\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.27, NNZs: 72036, Bias: -0.080771, T: 794145, Avg. loss: 0.009187\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 19.04, NNZs: 73987, Bias: -0.083275, T: 1058860, Avg. loss: 0.007055\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.41, NNZs: 75012, Bias: -0.084683, T: 1323575, Avg. loss: 0.005648\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 21.53, NNZs: 75709, Bias: -0.087936, T: 1588290, Avg. loss: 0.004711\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 22.44, NNZs: 76196, Bias: -0.089131, T: 1853005, Avg. loss: 0.003953\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 23.20, NNZs: 76520, Bias: -0.091186, T: 2117720, Avg. loss: 0.003324\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 23.87, NNZs: 76720, Bias: -0.092666, T: 2382435, Avg. loss: 0.003035\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 24.45, NNZs: 76843, Bias: -0.093634, T: 2647150, Avg. loss: 0.002680\n",
            "Total training time: 1.56 seconds.\n",
            "Convergence after 10 epochs took 1.56 seconds\n",
            "-- Epoch 1\n",
            "Norm: 14.40, NNZs: 69484, Bias: -0.088639, T: 264715, Avg. loss: 0.038350\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 18.87, NNZs: 78180, Bias: -0.102718, T: 529430, Avg. loss: 0.021794\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 21.92, NNZs: 81591, Bias: -0.111347, T: 794145, Avg. loss: 0.015896\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 24.26, NNZs: 83335, Bias: -0.115033, T: 1058860, Avg. loss: 0.012950\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 26.10, NNZs: 84284, Bias: -0.117197, T: 1323575, Avg. loss: 0.010633\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 27.62, NNZs: 84829, Bias: -0.119561, T: 1588290, Avg. loss: 0.008956\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 28.93, NNZs: 85238, Bias: -0.123096, T: 1853005, Avg. loss: 0.007961\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 30.04, NNZs: 85538, Bias: -0.123640, T: 2117720, Avg. loss: 0.006890\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 31.02, NNZs: 85676, Bias: -0.126250, T: 2382435, Avg. loss: 0.006154\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 31.90, NNZs: 85818, Bias: -0.126620, T: 2647150, Avg. loss: 0.005646\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 32.70, NNZs: 85913, Bias: -0.129951, T: 2911865, Avg. loss: 0.005248\n",
            "Total training time: 1.72 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 33.42, NNZs: 85979, Bias: -0.130485, T: 3176580, Avg. loss: 0.004809\n",
            "Total training time: 1.87 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 34.07, NNZs: 86084, Bias: -0.130922, T: 3441295, Avg. loss: 0.004439\n",
            "Total training time: 2.02 seconds.\n",
            "Convergence after 13 epochs took 2.02 seconds\n",
            "-- Epoch 1\n",
            "Norm: 12.47, NNZs: 55693, Bias: -0.075265, T: 264715, Avg. loss: 0.023069\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 15.83, NNZs: 61806, Bias: -0.085334, T: 529430, Avg. loss: 0.010389\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 17.89, NNZs: 63942, Bias: -0.089398, T: 794145, Avg. loss: 0.006572\n",
            "Total training time: 0.48 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 19.30, NNZs: 64823, Bias: -0.092130, T: 1058860, Avg. loss: 0.004633\n",
            "Total training time: 0.63 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 20.34, NNZs: 65291, Bias: -0.093156, T: 1323575, Avg. loss: 0.003396\n",
            "Total training time: 0.78 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 21.10, NNZs: 65531, Bias: -0.093935, T: 1588290, Avg. loss: 0.002565\n",
            "Total training time: 0.93 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 21.71, NNZs: 65661, Bias: -0.095920, T: 1853005, Avg. loss: 0.002041\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 22.20, NNZs: 65769, Bias: -0.096155, T: 2117720, Avg. loss: 0.001635\n",
            "Total training time: 1.24 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 22.58, NNZs: 65816, Bias: -0.096501, T: 2382435, Avg. loss: 0.001313\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 22.91, NNZs: 65846, Bias: -0.097363, T: 2647150, Avg. loss: 0.001102\n",
            "Total training time: 1.56 seconds.\n",
            "Convergence after 10 epochs took 1.56 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.43, NNZs: 49764, Bias: -0.077094, T: 264715, Avg. loss: 0.019974\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.65, NNZs: 57664, Bias: -0.090048, T: 529430, Avg. loss: 0.011413\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 15.81, NNZs: 60769, Bias: -0.094846, T: 794145, Avg. loss: 0.008400\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 17.47, NNZs: 62494, Bias: -0.100554, T: 1058860, Avg. loss: 0.006703\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 18.82, NNZs: 63513, Bias: -0.103932, T: 1323575, Avg. loss: 0.005652\n",
            "Total training time: 0.79 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 19.91, NNZs: 64150, Bias: -0.105957, T: 1588290, Avg. loss: 0.004775\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 20.84, NNZs: 64614, Bias: -0.108680, T: 1853005, Avg. loss: 0.004133\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 21.62, NNZs: 64849, Bias: -0.110535, T: 2117720, Avg. loss: 0.003559\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 22.34, NNZs: 65136, Bias: -0.111454, T: 2382435, Avg. loss: 0.003316\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 22.97, NNZs: 65298, Bias: -0.113464, T: 2647150, Avg. loss: 0.002942\n",
            "Total training time: 1.55 seconds.\n",
            "Convergence after 10 epochs took 1.55 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13.06, NNZs: 78606, Bias: -0.080544, T: 264715, Avg. loss: 0.036792\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.67, NNZs: 91750, Bias: -0.092085, T: 529430, Avg. loss: 0.023110\n",
            "Total training time: 0.33 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.90, NNZs: 97931, Bias: -0.098207, T: 794145, Avg. loss: 0.017301\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 23.38, NNZs: 101029, Bias: -0.104330, T: 1058860, Avg. loss: 0.014063\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 25.37, NNZs: 102893, Bias: -0.110122, T: 1323575, Avg. loss: 0.011601\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 27.02, NNZs: 104221, Bias: -0.111488, T: 1588290, Avg. loss: 0.009878\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 28.43, NNZs: 104930, Bias: -0.116605, T: 1853005, Avg. loss: 0.008641\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 29.63, NNZs: 105421, Bias: -0.118630, T: 2117720, Avg. loss: 0.007556\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 30.69, NNZs: 105784, Bias: -0.119202, T: 2382435, Avg. loss: 0.006740\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 31.62, NNZs: 106122, Bias: -0.121064, T: 2647150, Avg. loss: 0.006066\n",
            "Total training time: 1.57 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 32.46, NNZs: 106310, Bias: -0.124050, T: 2911865, Avg. loss: 0.005555\n",
            "Total training time: 1.73 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 33.22, NNZs: 106474, Bias: -0.124648, T: 3176580, Avg. loss: 0.005174\n",
            "Total training time: 1.88 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 33.91, NNZs: 106647, Bias: -0.125769, T: 3441295, Avg. loss: 0.004762\n",
            "Total training time: 2.03 seconds.\n",
            "Convergence after 13 epochs took 2.03 seconds\n",
            "-- Epoch 1\n",
            "Norm: 13.40, NNZs: 67938, Bias: -0.094066, T: 264715, Avg. loss: 0.036600\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 17.71, NNZs: 75915, Bias: -0.108444, T: 529430, Avg. loss: 0.022168\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 20.66, NNZs: 78964, Bias: -0.116116, T: 794145, Avg. loss: 0.016875\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 22.99, NNZs: 80429, Bias: -0.121194, T: 1058860, Avg. loss: 0.013800\n",
            "Total training time: 0.64 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 24.85, NNZs: 81253, Bias: -0.126305, T: 1323575, Avg. loss: 0.011476\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 26.41, NNZs: 81821, Bias: -0.129391, T: 1588290, Avg. loss: 0.010037\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 27.75, NNZs: 82204, Bias: -0.132910, T: 1853005, Avg. loss: 0.008805\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 28.94, NNZs: 82509, Bias: -0.134662, T: 2117720, Avg. loss: 0.007921\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 29.99, NNZs: 82706, Bias: -0.137741, T: 2382435, Avg. loss: 0.007193\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 30.96, NNZs: 82866, Bias: -0.139286, T: 2647150, Avg. loss: 0.006805\n",
            "Total training time: 1.56 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 31.80, NNZs: 82941, Bias: -0.139884, T: 2911865, Avg. loss: 0.006048\n",
            "Total training time: 1.71 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 32.58, NNZs: 83045, Bias: -0.141698, T: 3176580, Avg. loss: 0.005689\n",
            "Total training time: 1.86 seconds.\n",
            "Convergence after 12 epochs took 1.86 seconds\n",
            "-- Epoch 1\n",
            "Norm: 10.74, NNZs: 51872, Bias: -0.092597, T: 264715, Avg. loss: 0.016998\n",
            "Total training time: 0.18 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 13.37, NNZs: 57451, Bias: -0.105791, T: 529430, Avg. loss: 0.007456\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 14.97, NNZs: 59558, Bias: -0.112947, T: 794145, Avg. loss: 0.004729\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 16.08, NNZs: 60551, Bias: -0.117600, T: 1058860, Avg. loss: 0.003325\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 16.88, NNZs: 61015, Bias: -0.120704, T: 1323575, Avg. loss: 0.002468\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 17.50, NNZs: 61337, Bias: -0.122581, T: 1588290, Avg. loss: 0.001970\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 17.99, NNZs: 61511, Bias: -0.124165, T: 1853005, Avg. loss: 0.001533\n",
            "Total training time: 1.10 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 18.37, NNZs: 61623, Bias: -0.125445, T: 2117720, Avg. loss: 0.001207\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 18.71, NNZs: 61790, Bias: -0.126565, T: 2382435, Avg. loss: 0.001114\n",
            "Total training time: 1.40 seconds.\n",
            "Convergence after 9 epochs took 1.40 seconds\n",
            "-- Epoch 1\n",
            "Norm: 16.86, NNZs: 95710, Bias: 0.116711, T: 264715, Avg. loss: 0.050850\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 22.11, NNZs: 108527, Bias: 0.137257, T: 529430, Avg. loss: 0.028629\n",
            "Total training time: 0.34 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 25.74, NNZs: 113673, Bias: 0.147866, T: 794145, Avg. loss: 0.021072\n",
            "Total training time: 0.50 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 28.44, NNZs: 116537, Bias: 0.155625, T: 1058860, Avg. loss: 0.016378\n",
            "Total training time: 0.65 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 30.58, NNZs: 118267, Bias: 0.161311, T: 1323575, Avg. loss: 0.013587\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 32.33, NNZs: 119277, Bias: 0.168608, T: 1588290, Avg. loss: 0.011317\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 33.82, NNZs: 119898, Bias: 0.170888, T: 1853005, Avg. loss: 0.009761\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 35.08, NNZs: 120371, Bias: 0.174142, T: 2117720, Avg. loss: 0.008501\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 36.18, NNZs: 120761, Bias: 0.177688, T: 2382435, Avg. loss: 0.007618\n",
            "Total training time: 1.42 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 37.15, NNZs: 120984, Bias: 0.178352, T: 2647150, Avg. loss: 0.006760\n",
            "Total training time: 1.57 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 38.02, NNZs: 121196, Bias: 0.181772, T: 2911865, Avg. loss: 0.006183\n",
            "Total training time: 1.72 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 38.79, NNZs: 121330, Bias: 0.184111, T: 3176580, Avg. loss: 0.005556\n",
            "Total training time: 1.88 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 39.51, NNZs: 121475, Bias: 0.185371, T: 3441295, Avg. loss: 0.005211\n",
            "Total training time: 2.03 seconds.\n",
            "Convergence after 13 epochs took 2.03 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   15.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing to train_results.txt\n",
            "Writing to dev_results.txt\n",
            "Writing to test_results.txt\n",
            "Now run: python conlleval.py test_results.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_0NfgYnd71s",
        "colab_type": "code",
        "outputId": "64202eb1-5364-4984-b82b-7a53ea9f4cc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!python 'drive/My Drive/CIS-530/Homework 7/conlleval.py' 'drive/My Drive/CIS-530/Homework 7/Results/train_results.txt'"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 264715 tokens with 18797 phrases; found: 18934 phrases; correct: 18583.\n",
            "accuracy:  99.88%; precision:  98.15%; recall:  98.86%; FB1:  98.50\n",
            "              LOC: precision:  97.86%; recall:  98.72%; FB1:  98.29  4956\n",
            "             MISC: precision:  96.41%; recall:  97.70%; FB1:  97.05  2202\n",
            "              ORG: precision:  97.97%; recall:  98.73%; FB1:  98.35  7447\n",
            "              PER: precision:  99.65%; recall:  99.84%; FB1:  99.75  4329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SkPsQUgllwZ",
        "colab_type": "code",
        "outputId": "70903d87-0e81-4e49-ad26-a6df06cc4f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!python 'drive/My Drive/CIS-530/Homework 7/conlleval.py' 'drive/My Drive/CIS-530/Homework 7/Results/dev_results.txt'"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 52923 tokens with 4351 phrases; found: 4633 phrases; correct: 3097.\n",
            "accuracy:  95.90%; precision:  66.85%; recall:  71.18%; FB1:  68.94\n",
            "              LOC: precision:  62.92%; recall:  77.95%; FB1:  69.63  1219\n",
            "             MISC: precision:  39.14%; recall:  40.90%; FB1:  40.00  465\n",
            "              ORG: precision:  69.01%; recall:  69.18%; FB1:  69.10  1704\n",
            "              PER: precision:  78.07%; recall:  79.54%; FB1:  78.80  1245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-sV4EK3lnRk",
        "colab_type": "code",
        "outputId": "166120a7-fe1f-41e5-b239-e26d814e47f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!python 'drive/My Drive/CIS-530/Homework 7/conlleval.py' 'drive/My Drive/CIS-530/Homework 7/Results/test_results.txt'"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 51533 tokens with 3558 phrases; found: 3866 phrases; correct: 2639.\n",
            "accuracy:  96.89%; precision:  68.26%; recall:  74.17%; FB1:  71.09\n",
            "              LOC: precision:  75.61%; recall:  72.05%; FB1:  73.78  1033\n",
            "             MISC: precision:  38.14%; recall:  39.82%; FB1:  38.96  354\n",
            "              ORG: precision:  66.79%; recall:  75.71%; FB1:  70.97  1587\n",
            "              PER: precision:  74.33%; recall:  90.20%; FB1:  81.50  892\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}