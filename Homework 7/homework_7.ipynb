{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework_7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuf+P5GcXmFcv7qD1tOery",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%207/homework_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwJLStdTrMek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "21be5733-2348-410f-b8ff-dc61a9bad599"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWpGgT7v0-2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "315ade68-f77e-4d29-847f-aefa546d06d0"
      },
      "source": [
        "# from nltk.corpus import conll2002\n",
        "import nltk \n",
        "nltk.download('conll2002')\n",
        "from nltk.corpus import conll2002\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import pickle\n",
        "\n",
        "# Assignment 7: NER\n",
        "# Rebecca Iglesias-Flores and Shubhankar Patankar\n",
        "# This is just to help you get going. Feel free to\n",
        "# add to or modify any part of it."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wLNxPvR06Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hasDot(word):\n",
        "  return int('.' in word)\n",
        "\n",
        "def hasApost(word):\n",
        "  return int('\\'' in word)\n",
        "\n",
        "def hasHyph(word):\n",
        "  return int('-' in word)\n",
        "\n",
        "def hasNN(pos):\n",
        "  return int('NN' in pos)\n",
        "\n",
        "def isCap(word):\n",
        "    ## if first letter of the word is capitalized\n",
        "  return int(word[0].isupper()) \n",
        "\n",
        "def hasCap(word):\n",
        "    ## if any letter of the word is capitalized or not\n",
        "  return int(word.islower())\n",
        "\n",
        "def hasAcc(word):\n",
        "  for char in word:\n",
        "    if char in accents:\n",
        "      return int(True)\n",
        "  return int(False)\n",
        "\n",
        "def hasDig(word):\n",
        "  ## assigns a 1 if there's a digit in the word\n",
        "  return int(word.isalpha())\n",
        "\n",
        "def prefix(word):\n",
        "  return word[:4]\n",
        "\n",
        "def suffix(word):\n",
        "  return word[-4:]\n",
        "\n",
        "def getfeats(word, pos_tag, o):\n",
        "  \"\"\" This takes the word in question and\n",
        "  the offset with respect to the instance\n",
        "  word \"\"\"\n",
        "  o = str(o)\n",
        "  features = [\n",
        "              (o + 'word', word),\n",
        "              # TODO: add more features here.\n",
        "              (o + 'hasDot', hasDot(word)),\n",
        "              (o + 'hasApost', hasApost(word)),\n",
        "              (o + 'hasHyph', hasHyph(word)),\n",
        "              (o + 'hasNN', hasNN(pos_tag)),\n",
        "              (o + 'isCap', isCap(word)),\n",
        "              (o + 'hasCap', hasCap(word)),\n",
        "              # (o + 'hasAcc', hasAcc(word)),\n",
        "              (o + 'hasDig', hasDig(word)),\n",
        "              (o + 'prefix', prefix(word)),\n",
        "              (o + 'suffix', suffix(word))\n",
        "              ]\n",
        "  return features\n",
        "    \n",
        "def word2features(sent, i):\n",
        "  \"\"\" The function generates all features\n",
        "  for the word at position i in the\n",
        "  sentence.\"\"\"\n",
        "  features = []\n",
        "  # the window around the token\n",
        "  for o in [-1,0,1]:\n",
        "    if i+o >= 0 and i+o < len(sent):\n",
        "      word = sent[i+o][0]\n",
        "      pos_tag = sent[i+o][1]\n",
        "      featlist = getfeats(word, pos_tag, o)\n",
        "      features.extend(featlist)\n",
        "  \n",
        "  return dict(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulbN7T0LD5Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_sents = list(conll2002.iob_sents('esp.train'))\n",
        "# dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
        "# test_sents = list(conll2002.iob_sents('esp.testb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPe5FnsQC9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_feats = []\n",
        "# train_labels = []\n",
        "\n",
        "# for sent in train_sents:\n",
        "#   for i in range(len(sent)):\n",
        "#     feats = word2features(sent, i)\n",
        "#     train_feats.append(feats)\n",
        "#     train_labels.append(sent[i][-1])\n",
        "\n",
        "# vectorizer = DictVectorizer()\n",
        "# X_train = vectorizer.fit_transform(train_feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeDSiRxj6rYk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38f5cfde-7fa0-45c1-f921-eceac1b0226b"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  train_sents = list(conll2002.iob_sents('esp.train'))\n",
        "  train_feats = []\n",
        "  train_labels = []\n",
        "  for sent in train_sents:\n",
        "    for i in range(len(sent)):\n",
        "      feats = word2features(sent,i)\n",
        "      train_feats.append(feats)\n",
        "      train_labels.append(sent[i][-1])\n",
        "\n",
        "  vectorizer = DictVectorizer()\n",
        "  X_train = vectorizer.fit_transform(train_feats)\n",
        "\n",
        "  model = Perceptron(verbose = 1)\n",
        "  model.fit(X_train, train_labels)\n",
        "  pickle.dump(model, open('drive/My Drive/CIS-530/Homework 7/Results/model', 'wb'))\n",
        "\n",
        "  # TRAINING SET\n",
        "  y_pred_train = model.predict(X_train)  \n",
        "  j = 0\n",
        "  print(\"Writing to train_results.txt\")\n",
        "  # format is: word gold pred\n",
        "  outfile_path = 'drive/My Drive/CIS-530/Homework 7/Results/train_results.txt' \n",
        "  with open(outfile_path, \"w\") as out:\n",
        "    for sent in train_sents: \n",
        "      for i in range(len(sent)):\n",
        "        word = sent[i][0]\n",
        "        gold = sent[i][-1]\n",
        "        pred = y_pred_train[j]\n",
        "        j += 1\n",
        "        out.write(\"{}\\t{}\\t{}\\n\".format(word, gold, pred))\n",
        "    out.write(\"\\n\")\n",
        "\n",
        "  # DEVELOPMENT SET\n",
        "  dev_sents = list(conll2002.iob_sents('esp.testa'))\n",
        "  dev_feats = []\n",
        "  dev_labels = []\n",
        "  for sent in dev_sents:\n",
        "    for i in range(len(sent)):\n",
        "      feats = word2features(sent,i)\n",
        "      dev_feats.append(feats)\n",
        "      dev_labels.append(sent[i][-1])\n",
        "  X_dev = vectorizer.transform(dev_feats)\n",
        "  y_pred_dev = model.predict(X_dev)\n",
        "  j = 0\n",
        "  print(\"Writing to dev_results.txt\")\n",
        "  # format is: word gold pred\n",
        "  outfile_path = 'drive/My Drive/CIS-530/Homework 7/Results/dev_results.txt'\n",
        "  with open(outfile_path, \"w\") as out:\n",
        "    for sent in dev_sents: \n",
        "      for i in range(len(sent)):\n",
        "        word = sent[i][0]\n",
        "        gold = sent[i][-1]\n",
        "        pred = y_pred_dev[j]\n",
        "        j += 1\n",
        "        out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "    out.write(\"\\n\")\n",
        "\n",
        "  # TEST SET\n",
        "  test_sents = list(conll2002.iob_sents('esp.testb'))\n",
        "  test_feats = []\n",
        "  test_labels = []\n",
        "  for sent in test_sents:\n",
        "    for i in range(len(sent)):\n",
        "      feats = word2features(sent,i)\n",
        "      test_feats.append(feats)\n",
        "      test_labels.append(sent[i][-1])\n",
        "  X_test = vectorizer.transform(test_feats)\n",
        "  y_pred_test = model.predict(X_test)\n",
        "  j = 0\n",
        "  print(\"Writing to test_results.txt\")\n",
        "  # format is: word gold pred\n",
        "  outfile_path = 'drive/My Drive/CIS-530/Homework 7/Results/test_results.txt'\n",
        "  with open(outfile_path, \"w\") as out:\n",
        "    for sent in test_sents: \n",
        "      for i in range(len(sent)):\n",
        "        word = sent[i][0]\n",
        "        gold = sent[i][-1]\n",
        "        pred = y_pred_test[j]\n",
        "        j += 1\n",
        "        out.write(\"{}\\t{}\\t{}\\n\".format(word,gold,pred))\n",
        "    out.write(\"\\n\")\n",
        "\n",
        "  print(\"Now run: python conlleval.py test_results.txt\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 137.61, NNZs: 9027, Bias: -1.160000, T: 264715, Avg. loss: 0.076645\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 181.63, NNZs: 11503, Bias: -1.530000, T: 529430, Avg. loss: 0.053102\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 213.14, NNZs: 13020, Bias: -1.800000, T: 794145, Avg. loss: 0.046998\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 237.92, NNZs: 14061, Bias: -2.000000, T: 1058860, Avg. loss: 0.043453\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 257.73, NNZs: 14845, Bias: -2.110000, T: 1323575, Avg. loss: 0.040266\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 274.95, NNZs: 15328, Bias: -2.200000, T: 1588290, Avg. loss: 0.037948\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 289.40, NNZs: 15769, Bias: -2.310000, T: 1853005, Avg. loss: 0.036562\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 303.54, NNZs: 16160, Bias: -2.420000, T: 2117720, Avg. loss: 0.037257\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 316.86, NNZs: 16528, Bias: -2.480000, T: 2382435, Avg. loss: 0.037220\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 328.33, NNZs: 16831, Bias: -2.530000, T: 2647150, Avg. loss: 0.034271\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 337.93, NNZs: 17044, Bias: -2.570000, T: 2911865, Avg. loss: 0.034596\n",
            "Total training time: 1.54 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 348.05, NNZs: 17292, Bias: -2.670000, T: 3176580, Avg. loss: 0.034056\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 356.97, NNZs: 17467, Bias: -2.750000, T: 3441295, Avg. loss: 0.035886\n",
            "Total training time: 1.82 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 366.00, NNZs: 17644, Bias: -2.760000, T: 3706010, Avg. loss: 0.033626\n",
            "Total training time: 1.96 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 374.09, NNZs: 17855, Bias: -2.770000, T: 3970725, Avg. loss: 0.033788\n",
            "Total training time: 2.10 seconds.\n",
            "Convergence after 15 epochs took 2.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 117.52, NNZs: 7928, Bias: -0.670000, T: 264715, Avg. loss: 0.045572\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 155.14, NNZs: 10108, Bias: -0.910000, T: 529430, Avg. loss: 0.028337\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 182.63, NNZs: 11355, Bias: -1.030000, T: 794145, Avg. loss: 0.022806\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 202.88, NNZs: 12267, Bias: -1.090000, T: 1058860, Avg. loss: 0.019671\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 219.28, NNZs: 12780, Bias: -1.160000, T: 1323575, Avg. loss: 0.018050\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 232.56, NNZs: 13157, Bias: -1.220000, T: 1588290, Avg. loss: 0.016112\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 244.38, NNZs: 13593, Bias: -1.330000, T: 1853005, Avg. loss: 0.014898\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 254.06, NNZs: 13902, Bias: -1.330000, T: 2117720, Avg. loss: 0.014916\n",
            "Total training time: 1.11 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 263.95, NNZs: 14157, Bias: -1.390000, T: 2382435, Avg. loss: 0.014126\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 272.58, NNZs: 14339, Bias: -1.440000, T: 2647150, Avg. loss: 0.014035\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 281.21, NNZs: 14573, Bias: -1.470000, T: 2911865, Avg. loss: 0.013792\n",
            "Total training time: 1.51 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 287.90, NNZs: 14758, Bias: -1.530000, T: 3176580, Avg. loss: 0.013524\n",
            "Total training time: 1.65 seconds.\n",
            "Convergence after 12 epochs took 1.65 seconds\n",
            "-- Epoch 1\n",
            "Norm: 155.70, NNZs: 11508, Bias: -1.320000, T: 264715, Avg. loss: 0.082058\n",
            "Total training time: 0.17 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 200.20, NNZs: 14282, Bias: -1.720000, T: 529430, Avg. loss: 0.054295\n",
            "Total training time: 0.31 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 230.72, NNZs: 15806, Bias: -2.050000, T: 794145, Avg. loss: 0.045644\n",
            "Total training time: 0.45 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 254.13, NNZs: 16893, Bias: -2.250000, T: 1058860, Avg. loss: 0.040727\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 274.08, NNZs: 17663, Bias: -2.420000, T: 1323575, Avg. loss: 0.038463\n",
            "Total training time: 0.71 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 291.10, NNZs: 18261, Bias: -2.570000, T: 1588290, Avg. loss: 0.037638\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 306.06, NNZs: 18754, Bias: -2.670000, T: 1853005, Avg. loss: 0.035574\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 319.81, NNZs: 19217, Bias: -2.830000, T: 2117720, Avg. loss: 0.036028\n",
            "Total training time: 1.13 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 331.58, NNZs: 19561, Bias: -2.970000, T: 2382435, Avg. loss: 0.034539\n",
            "Total training time: 1.27 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 342.39, NNZs: 19766, Bias: -3.090000, T: 2647150, Avg. loss: 0.032959\n",
            "Total training time: 1.41 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 352.94, NNZs: 20078, Bias: -3.110000, T: 2911865, Avg. loss: 0.034324\n",
            "Total training time: 1.55 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 362.39, NNZs: 20272, Bias: -3.180000, T: 3176580, Avg. loss: 0.032316\n",
            "Total training time: 1.68 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 370.72, NNZs: 20466, Bias: -3.300000, T: 3441295, Avg. loss: 0.031844\n",
            "Total training time: 1.82 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 379.66, NNZs: 20694, Bias: -3.380000, T: 3706010, Avg. loss: 0.031977\n",
            "Total training time: 1.96 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 387.78, NNZs: 20813, Bias: -3.440000, T: 3970725, Avg. loss: 0.031562\n",
            "Total training time: 2.10 seconds.\n",
            "Convergence after 15 epochs took 2.10 seconds\n",
            "-- Epoch 1\n",
            "Norm: 133.60, NNZs: 10432, Bias: -1.040000, T: 264715, Avg. loss: 0.034512\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 167.87, NNZs: 12952, Bias: -1.300000, T: 529430, Avg. loss: 0.015083\n",
            "Total training time: 0.30 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 188.37, NNZs: 14270, Bias: -1.440000, T: 794145, Avg. loss: 0.009049\n",
            "Total training time: 0.44 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 202.13, NNZs: 14990, Bias: -1.530000, T: 1058860, Avg. loss: 0.006807\n",
            "Total training time: 0.58 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 213.68, NNZs: 15633, Bias: -1.590000, T: 1323575, Avg. loss: 0.005026\n",
            "Total training time: 0.72 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 222.35, NNZs: 16025, Bias: -1.620000, T: 1588290, Avg. loss: 0.004176\n",
            "Total training time: 0.85 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 229.22, NNZs: 16325, Bias: -1.640000, T: 1853005, Avg. loss: 0.003735\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 236.21, NNZs: 16544, Bias: -1.680000, T: 2117720, Avg. loss: 0.003272\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 242.33, NNZs: 16784, Bias: -1.710000, T: 2382435, Avg. loss: 0.003338\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 247.05, NNZs: 16931, Bias: -1.740000, T: 2647150, Avg. loss: 0.002989\n",
            "Total training time: 1.39 seconds.\n",
            "Convergence after 10 epochs took 1.39 seconds\n",
            "-- Epoch 1\n",
            "Norm: 113.22, NNZs: 7131, Bias: -0.850000, T: 264715, Avg. loss: 0.044077\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 151.70, NNZs: 9322, Bias: -1.230000, T: 529430, Avg. loss: 0.029993\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 177.06, NNZs: 10568, Bias: -1.440000, T: 794145, Avg. loss: 0.023483\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 196.90, NNZs: 11422, Bias: -1.640000, T: 1058860, Avg. loss: 0.021230\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 212.85, NNZs: 11968, Bias: -1.870000, T: 1323575, Avg. loss: 0.019049\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 228.20, NNZs: 12495, Bias: -1.990000, T: 1588290, Avg. loss: 0.018381\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 241.80, NNZs: 12914, Bias: -2.100000, T: 1853005, Avg. loss: 0.018182\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 253.28, NNZs: 13208, Bias: -2.260000, T: 2117720, Avg. loss: 0.016154\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 264.29, NNZs: 13514, Bias: -2.350000, T: 2382435, Avg. loss: 0.015750\n",
            "Total training time: 1.25 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 273.77, NNZs: 13792, Bias: -2.410000, T: 2647150, Avg. loss: 0.015494\n",
            "Total training time: 1.38 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 282.30, NNZs: 14008, Bias: -2.470000, T: 2911865, Avg. loss: 0.015976\n",
            "Total training time: 1.53 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 289.76, NNZs: 14208, Bias: -2.550000, T: 3176580, Avg. loss: 0.016215\n",
            "Total training time: 1.67 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 297.18, NNZs: 14336, Bias: -2.610000, T: 3441295, Avg. loss: 0.015288\n",
            "Total training time: 1.81 seconds.\n",
            "Convergence after 13 epochs took 1.81 seconds\n",
            "-- Epoch 1\n",
            "Norm: 145.83, NNZs: 13724, Bias: -1.090000, T: 264715, Avg. loss: 0.084934\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 198.72, NNZs: 17752, Bias: -1.470000, T: 529430, Avg. loss: 0.061642\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 235.52, NNZs: 20070, Bias: -1.830000, T: 794145, Avg. loss: 0.051013\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 265.17, NNZs: 21695, Bias: -2.030000, T: 1058860, Avg. loss: 0.044304\n",
            "Total training time: 0.55 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 289.59, NNZs: 23056, Bias: -2.260000, T: 1323575, Avg. loss: 0.040482\n",
            "Total training time: 0.68 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 310.68, NNZs: 24021, Bias: -2.350000, T: 1588290, Avg. loss: 0.036759\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 328.80, NNZs: 24801, Bias: -2.530000, T: 1853005, Avg. loss: 0.036602\n",
            "Total training time: 0.95 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 344.43, NNZs: 25337, Bias: -2.660000, T: 2117720, Avg. loss: 0.034688\n",
            "Total training time: 1.07 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 359.68, NNZs: 25860, Bias: -2.790000, T: 2382435, Avg. loss: 0.034361\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 373.82, NNZs: 26326, Bias: -2.880000, T: 2647150, Avg. loss: 0.032897\n",
            "Total training time: 1.34 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 386.90, NNZs: 26698, Bias: -3.010000, T: 2911865, Avg. loss: 0.031544\n",
            "Total training time: 1.47 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 398.18, NNZs: 27118, Bias: -3.060000, T: 3176580, Avg. loss: 0.030949\n",
            "Total training time: 1.60 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 409.38, NNZs: 27419, Bias: -3.150000, T: 3441295, Avg. loss: 0.030674\n",
            "Total training time: 1.73 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 419.13, NNZs: 27635, Bias: -3.220000, T: 3706010, Avg. loss: 0.030430\n",
            "Total training time: 1.86 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 428.87, NNZs: 27919, Bias: -3.310000, T: 3970725, Avg. loss: 0.029092\n",
            "Total training time: 2.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 437.40, NNZs: 28066, Bias: -3.400000, T: 4235440, Avg. loss: 0.028793\n",
            "Total training time: 2.13 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 445.96, NNZs: 28322, Bias: -3.460000, T: 4500155, Avg. loss: 0.029634\n",
            "Total training time: 2.27 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 454.43, NNZs: 28490, Bias: -3.590000, T: 4764870, Avg. loss: 0.029898\n",
            "Total training time: 2.40 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 462.40, NNZs: 28695, Bias: -3.600000, T: 5029585, Avg. loss: 0.028847\n",
            "Total training time: 2.54 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 469.89, NNZs: 28864, Bias: -3.680000, T: 5294300, Avg. loss: 0.029015\n",
            "Total training time: 2.67 seconds.\n",
            "Convergence after 20 epochs took 2.67 seconds\n",
            "-- Epoch 1\n",
            "Norm: 157.90, NNZs: 13062, Bias: -1.610000, T: 264715, Avg. loss: 0.101446\n",
            "Total training time: 0.16 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 205.97, NNZs: 16647, Bias: -2.200000, T: 529430, Avg. loss: 0.076478\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 242.22, NNZs: 18719, Bias: -2.700000, T: 794145, Avg. loss: 0.066459\n",
            "Total training time: 0.43 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 270.93, NNZs: 20109, Bias: -2.990000, T: 1058860, Avg. loss: 0.059168\n",
            "Total training time: 0.57 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 296.24, NNZs: 21016, Bias: -3.190000, T: 1323575, Avg. loss: 0.056478\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 316.06, NNZs: 21689, Bias: -3.410000, T: 1588290, Avg. loss: 0.053581\n",
            "Total training time: 0.84 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 333.24, NNZs: 22237, Bias: -3.580000, T: 1853005, Avg. loss: 0.052416\n",
            "Total training time: 0.98 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 350.15, NNZs: 22649, Bias: -3.730000, T: 2117720, Avg. loss: 0.050040\n",
            "Total training time: 1.12 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 364.78, NNZs: 23088, Bias: -3.930000, T: 2382435, Avg. loss: 0.049722\n",
            "Total training time: 1.26 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 378.57, NNZs: 23497, Bias: -4.100000, T: 2647150, Avg. loss: 0.048737\n",
            "Total training time: 1.40 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 391.06, NNZs: 23789, Bias: -4.220000, T: 2911865, Avg. loss: 0.047149\n",
            "Total training time: 1.54 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 402.24, NNZs: 24065, Bias: -4.290000, T: 3176580, Avg. loss: 0.047640\n",
            "Total training time: 1.67 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 413.31, NNZs: 24293, Bias: -4.440000, T: 3441295, Avg. loss: 0.047154\n",
            "Total training time: 1.81 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 423.73, NNZs: 24457, Bias: -4.540000, T: 3706010, Avg. loss: 0.044888\n",
            "Total training time: 1.94 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 433.09, NNZs: 24636, Bias: -4.620000, T: 3970725, Avg. loss: 0.045568\n",
            "Total training time: 2.08 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 442.38, NNZs: 24791, Bias: -4.730000, T: 4235440, Avg. loss: 0.045051\n",
            "Total training time: 2.21 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 450.82, NNZs: 24940, Bias: -4.800000, T: 4500155, Avg. loss: 0.045493\n",
            "Total training time: 2.34 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 458.13, NNZs: 25069, Bias: -4.840000, T: 4764870, Avg. loss: 0.045379\n",
            "Total training time: 2.47 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 465.67, NNZs: 25168, Bias: -4.960000, T: 5029585, Avg. loss: 0.044965\n",
            "Total training time: 2.60 seconds.\n",
            "Convergence after 19 epochs took 2.60 seconds\n",
            "-- Epoch 1\n",
            "Norm: 133.33, NNZs: 10594, Bias: -1.660000, T: 264715, Avg. loss: 0.031305\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 163.34, NNZs: 13180, Bias: -2.140000, T: 529430, Avg. loss: 0.013901\n",
            "Total training time: 0.28 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 181.05, NNZs: 14358, Bias: -2.360000, T: 794145, Avg. loss: 0.008527\n",
            "Total training time: 0.41 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 194.31, NNZs: 15210, Bias: -2.600000, T: 1058860, Avg. loss: 0.007532\n",
            "Total training time: 0.54 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 205.67, NNZs: 15872, Bias: -2.770000, T: 1323575, Avg. loss: 0.006279\n",
            "Total training time: 0.67 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 214.20, NNZs: 16292, Bias: -2.880000, T: 1588290, Avg. loss: 0.005562\n",
            "Total training time: 0.81 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 221.85, NNZs: 16594, Bias: -2.980000, T: 1853005, Avg. loss: 0.005050\n",
            "Total training time: 0.94 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 228.14, NNZs: 16853, Bias: -3.060000, T: 2117720, Avg. loss: 0.004759\n",
            "Total training time: 1.08 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 235.27, NNZs: 17159, Bias: -3.140000, T: 2382435, Avg. loss: 0.004561\n",
            "Total training time: 1.21 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 239.99, NNZs: 17324, Bias: -3.180000, T: 2647150, Avg. loss: 0.004450\n",
            "Total training time: 1.35 seconds.\n",
            "Convergence after 10 epochs took 1.35 seconds\n",
            "-- Epoch 1\n",
            "Norm: 198.31, NNZs: 20900, Bias: 2.900000, T: 264715, Avg. loss: 0.116434\n",
            "Total training time: 0.15 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 257.97, NNZs: 25526, Bias: 3.720000, T: 529430, Avg. loss: 0.067274\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 299.49, NNZs: 28097, Bias: 4.290000, T: 794145, Avg. loss: 0.052442\n",
            "Total training time: 0.42 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 329.10, NNZs: 29695, Bias: 4.690000, T: 1058860, Avg. loss: 0.043090\n",
            "Total training time: 0.56 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 355.88, NNZs: 30972, Bias: 5.020000, T: 1323575, Avg. loss: 0.038633\n",
            "Total training time: 0.69 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 377.24, NNZs: 31884, Bias: 5.270000, T: 1588290, Avg. loss: 0.034152\n",
            "Total training time: 0.83 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 395.32, NNZs: 32614, Bias: 5.520000, T: 1853005, Avg. loss: 0.033797\n",
            "Total training time: 0.96 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 412.01, NNZs: 33164, Bias: 5.710000, T: 2117720, Avg. loss: 0.030977\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 427.06, NNZs: 33678, Bias: 5.930000, T: 2382435, Avg. loss: 0.030714\n",
            "Total training time: 1.23 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 440.99, NNZs: 34163, Bias: 6.050000, T: 2647150, Avg. loss: 0.029050\n",
            "Total training time: 1.37 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 453.58, NNZs: 34523, Bias: 6.210000, T: 2911865, Avg. loss: 0.029532\n",
            "Total training time: 1.50 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 465.68, NNZs: 34863, Bias: 6.370000, T: 3176580, Avg. loss: 0.027246\n",
            "Total training time: 1.64 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 476.41, NNZs: 35225, Bias: 6.540000, T: 3441295, Avg. loss: 0.027007\n",
            "Total training time: 1.78 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 487.25, NNZs: 35494, Bias: 6.690000, T: 3706010, Avg. loss: 0.027251\n",
            "Total training time: 1.92 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 496.83, NNZs: 35754, Bias: 6.790000, T: 3970725, Avg. loss: 0.026395\n",
            "Total training time: 2.05 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 505.92, NNZs: 35986, Bias: 6.900000, T: 4235440, Avg. loss: 0.025465\n",
            "Total training time: 2.20 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 514.44, NNZs: 36147, Bias: 6.950000, T: 4500155, Avg. loss: 0.026748\n",
            "Total training time: 2.34 seconds.\n",
            "Convergence after 17 epochs took 2.34 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   18.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing to train_results.txt\n",
            "Writing to dev_results.txt\n",
            "Writing to test_results.txt\n",
            "Now run: python conlleval.py test_results.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_0NfgYnd71s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e77f0eb5-edab-4f2d-abdc-6384cd97fe31"
      },
      "source": [
        "!python 'drive/My Drive/CIS-530/Homework 7/conlleval.py' 'drive/My Drive/CIS-530/Homework 7/Results/train_results.txt'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 264715 tokens with 18797 phrases; found: 21168 phrases; correct: 16387.\n",
            "accuracy:  98.52%; precision:  77.41%; recall:  87.18%; FB1:  82.01\n",
            "              LOC: precision:  75.34%; recall:  91.55%; FB1:  82.66  5970\n",
            "             MISC: precision:  61.25%; recall:  77.68%; FB1:  68.49  2756\n",
            "              ORG: precision:  79.33%; recall:  80.51%; FB1:  79.92  7500\n",
            "              PER: precision:  86.02%; recall:  98.38%; FB1:  91.78  4942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SkPsQUgllwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "61f4fade-fabf-44f6-9d4c-500040df3006"
      },
      "source": [
        "!python 'drive/My Drive/CIS-530/Homework 7/conlleval.py' 'drive/My Drive/CIS-530/Homework 7/Results/dev_results.txt'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 52923 tokens with 4351 phrases; found: 5397 phrases; correct: 2703.\n",
            "accuracy:  93.99%; precision:  50.08%; recall:  62.12%; FB1:  55.46\n",
            "              LOC: precision:  46.31%; recall:  75.30%; FB1:  57.35  1600\n",
            "             MISC: precision:  27.53%; recall:  36.63%; FB1:  31.44  592\n",
            "              ORG: precision:  57.74%; recall:  50.24%; FB1:  53.73  1479\n",
            "              PER: precision:  54.75%; recall:  77.33%; FB1:  64.11  1726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-sV4EK3lnRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "66476707-06ba-4f66-e16d-940913b5ce9a"
      },
      "source": [
        "!python 'drive/My Drive/CIS-530/Homework 7/conlleval.py' 'drive/My Drive/CIS-530/Homework 7/Results/test_results.txt'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 51533 tokens with 3558 phrases; found: 4417 phrases; correct: 2389.\n",
            "accuracy:  95.55%; precision:  54.09%; recall:  67.14%; FB1:  59.91\n",
            "              LOC: precision:  59.19%; recall:  71.31%; FB1:  64.69  1306\n",
            "             MISC: precision:  25.33%; recall:  39.53%; FB1:  30.88  529\n",
            "              ORG: precision:  59.67%; recall:  61.29%; FB1:  60.47  1438\n",
            "              PER: precision:  54.55%; recall:  84.90%; FB1:  66.42  1144\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}