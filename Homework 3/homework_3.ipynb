{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Part 0: Utility Functions\n",
    "################################################################################\n",
    "\n",
    "COUNTRY_CODES = ['af', 'cn', 'de', 'fi', 'fr', 'in', 'ir', 'pk', 'za']\n",
    "\n",
    "def start_pad(n):\n",
    "    ''' Returns a padding string of length n to append to the front of text\n",
    "        as a pre-processing step to building n-grams '''\n",
    "    return '~' * n\n",
    "\n",
    "def ngrams(n, text):\n",
    "    ''' Returns the ngrams of the text as tuples where the first element is\n",
    "        the length-n context and the second is the character '''\n",
    "    text = start_pad(n) + text\n",
    "    grams = []\n",
    "    for j in range(len(text)-n):\n",
    "        context = text[j:j+n]\n",
    "        char = text[j+n]\n",
    "        grams.append((context, char))\n",
    "    return grams\n",
    "\n",
    "def create_ngram_model(model_class, path, n = 2, k = 0):\n",
    "    ''' Creates and returns a new n-gram model trained on the city names\n",
    "        found in the path file '''\n",
    "    model = model_class(n, k)\n",
    "    with open(path, encoding = 'utf-8', errors = 'ignore') as f:\n",
    "        model.update(f.read())\n",
    "    return model\n",
    "\n",
    "def create_ngram_model_lines(model_class, path, n = 2, k = 0):\n",
    "    ''' Creates and returns a new n-gram model trained on the city names\n",
    "        found in the path file '''\n",
    "    model = model_class(n, k)\n",
    "    with open(path, encoding = 'utf-8', errors = 'ignore') as f:\n",
    "        for line in f:\n",
    "            model.update(line.strip())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Part 1: Basic N-Gram Model\n",
    "################################################################################\n",
    "\n",
    "class NgramModel(object):\n",
    "    ''' A basic n-gram model using add-k smoothing '''\n",
    "\n",
    "    def __init__(self, n, k):\n",
    "        self.n = n # order of n-gram model\n",
    "        self.vocab = set() # initialize vocabulary\n",
    "        self.context_counts = defaultdict(lambda:0) # frequency of contexts\n",
    "        self.sequence_counts = defaultdict(lambda:0) # frequency of (context, char) sequences\n",
    "        self.k = k # smoothing parameter\n",
    "\n",
    "    def get_vocab(self):\n",
    "        ''' Returns the set of characters in the vocab '''\n",
    "        return self.vocab\n",
    "\n",
    "    def update(self, text):\n",
    "        ''' Updates the model n-grams based on text '''\n",
    "        all_ngrams = ngrams(self.n, text)\n",
    "        for (context, char) in all_ngrams:\n",
    "            self.vocab.add(char)\n",
    "            self.context_counts[context] += 1 # increment the context count\n",
    "            self.sequence_counts[(context, char)] += 1 # increment the (context, character) sequence count\n",
    "\n",
    "    def prob(self, context, char):\n",
    "        ''' Returns the probability of char appearing after context '''\n",
    "        # print(self.n)\n",
    "        if context in self.context_counts.keys():\n",
    "            denominator = self.context_counts[context] # frequency of context followed by any token\n",
    "            numerator = self.sequence_counts[(context, char)] # frequency of exact (context, character) sequence\n",
    "            prob = (numerator + self.k)/(denominator + (self.k * len(self.vocab)))\n",
    "            return prob\n",
    "        else:\n",
    "            return 1/len(self.vocab)\n",
    "                \n",
    "    def random_char(self, context):\n",
    "        ''' Returns a random character based on the given context and the \n",
    "            n-grams learned by this model '''\n",
    "        r = random.random()\n",
    "        pre_sum = 0\n",
    "        for i, char in enumerate(sorted(self.vocab)):\n",
    "            # pre_sum is sum of probabilities up to, but excluding, the current token\n",
    "            post_sum = pre_sum + self.prob(context, char)\n",
    "            # post_sum also includes the probability of the current token\n",
    "            if pre_sum <= r < post_sum:\n",
    "                return char\n",
    "            pre_sum = post_sum\n",
    "                \n",
    "    def random_text(self, length):\n",
    "        output_text = ''\n",
    "        all_context = start_pad(self.n) # keep a running context list initialized with '~'s\n",
    "        for i in range(length):\n",
    "            curr_context = all_context[len(all_context)-self.n:] # extract context from running context list\n",
    "            next_char = self.random_char(curr_context)\n",
    "            output_text += next_char\n",
    "            all_context += next_char\n",
    "        return output_text\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        ''' Returns the perplexity of text based on the n-grams learned by\n",
    "            this model '''\n",
    "        m = len(text)\n",
    "        all_ngrams = ngrams(self.n, text)\n",
    "        log_prob_sum = 0\n",
    "        for (context, char) in all_ngrams:\n",
    "            prob = self.prob(context, char)\n",
    "            if prob == 0:\n",
    "                return float('inf')\n",
    "            log_prob_sum += math.log(prob)\n",
    "        perplexity = math.exp(-1/(m) * log_prob_sum)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_lambdas(n):\n",
    "    lambdas = {}\n",
    "    for order in range(n + 1):\n",
    "        lambdas[order] = 1/(n + 1)\n",
    "    return lambdas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def increasing_lambdas(n):\n",
    "    lambdas = {}\n",
    "    lambdas_array = np.linspace(0, 1, n + 1)\n",
    "    lambdas_array = lambdas_array/sum(lambdas_array) # ensure sum is 1\n",
    "    for order, weight in enumerate(lambdas_array):\n",
    "        lambdas[order] = weight\n",
    "    return lambdas\n",
    "\n",
    "def decreasing_lambdas(n):\n",
    "    lambdas = {}\n",
    "    lambdas_array = np.linspace(0, 1, n + 1)\n",
    "    lambdas_array = np.flip(lambdas_array/sum(lambdas_array)) # ensure sum is 1\n",
    "    for order, weight in enumerate(lambdas_array):\n",
    "        lambdas[order] = weight\n",
    "    return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Part 2: N-Gram Model with Interpolation\n",
    "################################################################################\n",
    "\n",
    "class NgramModelWithInterpolation(NgramModel):\n",
    "    ''' An n-gram model with interpolation '''\n",
    "\n",
    "    def __init__(self, n, k):\n",
    "        self.n = n # highest order n-gram model\n",
    "        self.models = {} # initialize empty dictionary for NgramModels\n",
    "        # self.weights = uniform_lambdas(n) # lambdas corresponding to each NgramModel\n",
    "        # self.weights = increasing_lambdas(n) # lambdas corresponding to each NgramModel\n",
    "        self.weights = decreasing_lambdas(n) # lambdas corresponding to each NgramModel\n",
    "        print(self.weights)\n",
    "        for order in range(n + 1): # extra model accounts for zeroth order\n",
    "            self.models[order] = NgramModel(order, k)\n",
    "        self.k = k # smoothing parameter\n",
    "            \n",
    "    def get_vocab(self):\n",
    "        vocab = set()\n",
    "        for order in range(self.n + 1):\n",
    "            model = self.models[order]\n",
    "            vocab = vocab.union(model.get_vocab()) # merge vocabularies \n",
    "        return vocab\n",
    "\n",
    "    def update(self, text):\n",
    "        for order in range(self.n + 1):\n",
    "            model = self.models[order]\n",
    "            model.update(text)\n",
    "\n",
    "    def prob(self, context, char):\n",
    "        prob = 0\n",
    "        for order in range(self.n + 1):\n",
    "            model = self.models[order]\n",
    "            weight = self.weights[order]\n",
    "            if model.n == 0:\n",
    "                sliced_context = ''\n",
    "            else:\n",
    "                sliced_context = context[-model.n:]\n",
    "            prob += weight * model.prob(sliced_context, char)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5, 1: 0.3333333333333333, 2: 0.16666666666666666, 3: 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.023420948467447"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "k = 1\n",
    "m = create_ngram_model(NgramModelWithInterpolation, 'Data/shakespeare_input.txt', n, k)\n",
    "path = '/Users/sppatankar/Desktop/CIS-530/Homework 3/Data/nytimes_article.txt'\n",
    "with open(path, encoding = 'utf-8', errors = 'ignore') as f:\n",
    "        text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Part 3: Your N-Gram Model Experimentation\n",
    "################################################################################\n",
    "\n",
    "def accuracy(all_pred_labels, all_true_labels):\n",
    "    num_correct = 0\n",
    "    for i, pred in enumerate(all_pred_labels):\n",
    "        if pred == all_true_labels[i]:\n",
    "            num_correct += 1\n",
    "    accuracy = num_correct/len(all_true_labels)\n",
    "    return accuracy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # training data\n",
    "    all_training_data = '/Users/sppatankar/Desktop/CIS-530/Homework 3/Data/train'\n",
    "    models = {}\n",
    "    n = 4\n",
    "    k = 1 \n",
    "    for filename in os.listdir(all_training_data):\n",
    "        filepath = os.path.join(all_training_data, filename) # of the form 'country_code.txt'\n",
    "        country = filename.split('.')[0] # split on the period to isolate country code \n",
    "        models[country] = create_ngram_model_lines(NgramModelWithInterpolation, filepath, n, k)\n",
    "    \n",
    "    # validation data\n",
    "    all_validation_data = '/Users/sppatankar/Desktop/CIS-530/Homework 3/Data/val'\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "    for filename in os.listdir(all_validation_data):\n",
    "        filepath = os.path.join(all_validation_data, filename) \n",
    "        true_country = filename.split('.')[0]\n",
    "        with open(filepath, encoding = 'utf-8', errors = 'ignore') as f:\n",
    "            for line in f:\n",
    "                perplexities = {}\n",
    "                for country in models.keys(): # loop over the models for all countries\n",
    "                    perplexities[country] = models[country].perplexity(line)\n",
    "                # find the model that has the lowest perplexity and assign prediction\n",
    "                # print(line, perplexities)\n",
    "                pred_country = min(perplexities, key = perplexities.get)\n",
    "                all_true_labels.append(true_country)\n",
    "                all_pred_labels.append(pred_country)\n",
    "                \n",
    "    print('Development Accuracy = %f\\n' % accuracy(all_pred_labels, all_true_labels))\n",
    "                \n",
    "    test_file = '/Users/sppatankar/Desktop/CIS-530/Homework 3/cities_test.txt'\n",
    "    all_pred_labels_test = [] \n",
    "    with open(test_file, encoding = 'utf-8', errors = 'ignore') as f:\n",
    "        for line in f:\n",
    "            perplexities = {}\n",
    "            for country in models.keys(): \n",
    "                perplexities[country] = models[country].perplexity(line)\n",
    "            pred_country = min(perplexities, key = perplexities.get)\n",
    "            all_pred_labels_test.append(pred_country)\n",
    "    with open('test_labels.txt', 'w') as f:\n",
    "        for label in all_pred_labels_test:\n",
    "            f.write('%s\\n' % label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
