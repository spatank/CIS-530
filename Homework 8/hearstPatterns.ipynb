{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hearstPatterns.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLuz5W6EkOI6KC7KAJzkgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%208/hearstPatterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_COTZ3LO4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "de039098-4edd-4857-ddfa-1148b3266cb0"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XupaEdyCKraK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HearstPatterns(object):\n",
        "  \n",
        "  def __init__(self, extended=False):\n",
        "    self.__chunk_patterns = r\"\"\" #  helps us find noun phrase chunks\n",
        "            NP: {<DT>?<JJ.*>*<NN.*>+}\n",
        "                {<NN.*>+}\n",
        "            \"\"\"\n",
        "            \n",
        "    # create a chunk parser\n",
        "    self.__np_chunker = nltk.RegexpParser(self.__chunk_patterns)\n",
        "\n",
        "    # now define the Hearst patterns\n",
        "    # format is <hearst-pattern>, <hypernym_location>\n",
        "    # so, what this means is that if you apply the first pattern,\n",
        "\n",
        "    # self.__hearst_patterns = [\n",
        "    #         (\"(NP_\\w+ (, )?such as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "\n",
        "    #         ''' IMPLEMENT ADDITIONAL HEARST PATTERNS HERE '''\n",
        "    #     ]\n",
        "    \n",
        "    self.__hearst_patterns = [\n",
        "            (\"(NP_\\w+ (, )?such as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\")\n",
        "        ]\n",
        "        \n",
        "    if extended:\n",
        "      self.__hearst_patterns.extend([\n",
        "            (\"(NP_\\w+ (, )?such as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            ''' IMPLEMENT ADDITIONAL PATTERNS HERE '''\n",
        "            ])\n",
        "\n",
        "    self.__pos_tagger = PerceptronTagger()\n",
        "    \n",
        "  def prepare(self, rawtext):\n",
        "    # To process text in NLTK format\n",
        "    sentences = nltk.sent_tokenize(rawtext.strip())\n",
        "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    sentences = [self.__pos_tagger.tag(sent) for sent in sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "  def chunk(self, rawtext):\n",
        "    sentences = self.prepare(rawtext.strip())\n",
        "\n",
        "    all_chunks = []\n",
        "    for sentence in sentences:\n",
        "      chunks = self.__np_chunker.parse(sentence)\n",
        "      all_chunks.append(self.prepare_chunks(chunks))\n",
        "\n",
        "    # two or more NPs next to each other should be merged into a single NP,\n",
        "    # find any N consecutive NP_ and merge them into one...\n",
        "    # Eg: \"NP_foo NP_bar blah blah\" becomes \"NP_foo_bar blah blah\"\n",
        "    all_sentences = []\n",
        "    for raw_sentence in all_chunks:\n",
        "      sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",\n",
        "                        lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),\n",
        "                        raw_sentence)\n",
        "      all_sentences.append(sentence)\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "  def prepare_chunks(self, chunks):\n",
        "    # If chunk is NP, start with NP_ and join tokens in chunk with _\n",
        "    # Else just keep the token as it is\n",
        "\n",
        "    terms = []\n",
        "    for chunk in chunks:\n",
        "      label = None\n",
        "      try:\n",
        "        # gross hack to see if the chunk is simply a word or a NP, as\n",
        "        # we want. But non-NP fail on this method call\n",
        "        label = chunk.label()\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      if label is None:  # means one word...\n",
        "        token = chunk[0]\n",
        "        terms.append(token)\n",
        "      else:\n",
        "        np = \"NP_\"+\"_\".join([a[0] for a in chunk])\n",
        "        terms.append(np)\n",
        "    \n",
        "    return ' '.join(terms)\n",
        "\n",
        "  def find_hyponyms(self, rawtext):\n",
        "    \n",
        "    hypo_hypernyms = []\n",
        "    np_tagged_sentences = self.chunk(rawtext)\n",
        "    \n",
        "    for sentence in np_tagged_sentences:\n",
        "      for (hearst_pattern, parser) in self.__hearst_patterns:\n",
        "        matches = re.search(hearst_pattern, sentence)\n",
        "        if matches:\n",
        "          match_str = matches.group(0)\n",
        "          nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
        "          if parser == \"first\":\n",
        "            hypernym = nps[0]\n",
        "            hyponyms = nps[1:]\n",
        "          else:\n",
        "            hypernym = nps[-1]\n",
        "            hyponyms = nps[:-1]\n",
        "            \n",
        "          for i in range(len(hyponyms)):\n",
        "            hypo_hypernyms.append(\n",
        "                (self.clean_hyponym_term(hyponyms[i]),\n",
        "                 self.clean_hyponym_term(hypernym)))\n",
        "\n",
        "    return hypo_hypernyms\n",
        "\n",
        "  def clean_hyponym_term(self, term):\n",
        "    return term.replace(\"NP_\", \"\").replace(\"_\", \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W3pfXRZNKY-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c28db59-53a2-48a2-a575-a81aafba8429"
      },
      "source": [
        "if __name__=='__main__':\n",
        "  hp = HearstPatterns(extended=False)\n",
        "  text = 'I like to listen to music from musical genres such as blues, rock and jazz.'\n",
        "  hps = hp.find_hyponyms(text)\n",
        "  print(hps)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('blues', 'musical genres'), ('rock', 'musical genres'), ('jazz', 'musical genres')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}