{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hearstPatterns.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOedK97SJmXBBql9/DnrXPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%208/hearstPatterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_COTZ3LO4d",
        "colab_type": "code",
        "outputId": "cbdfb9b5-e247-4393-b812-9734967da36e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XupaEdyCKraK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HearstPatterns(object):\n",
        "  \n",
        "  def __init__(self, extended=False):\n",
        "    self.__chunk_patterns = r\"\"\" #  helps us find noun phrase chunks\n",
        "            NP: {<DT>?<JJ.*>*<NN.*>+}\n",
        "                {<NN.*>+}\n",
        "            \"\"\"\n",
        "            \n",
        "    # create a chunk parser\n",
        "    self.__np_chunker = nltk.RegexpParser(self.__chunk_patterns)\n",
        "\n",
        "    # now define the Hearst patterns\n",
        "    # format is <hearst-pattern>, <hypernym_location>\n",
        "    # so, what this means is that if you apply the first pattern,\n",
        "    \n",
        "    self.__hearst_patterns = [\n",
        "            (\"(NP_[\\w\\-]+ (, )?such as (NP_[\\w\\-]+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            (\"(NP_such_[\\w\\-]+ as (NP_[\\w\\-]+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            # `such' (JJ) is being merged with the hypernym, workaround above\n",
        "            (\"((NP_[\\w\\-]+ ?(, )?)+(and |or )?NP_other_[\\w\\-]+)\", \"second\"),\n",
        "            (\"(NP_[\\w\\-]+ (, )?including (NP_[\\w\\-]+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            (\"(NP_[\\w\\-]+ (, )?especially (NP_[\\w\\-]+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            (\"(NP_[\\w]+ (, )?has been (NP_[\\w]+ ? (, )?(and |or )?)+)\", \"second\"),\n",
        "            (\"(NP_[\\w]+ (, )?are otherwise known as (NP_[\\w]+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            (\"(NP_[\\w]+ (, )?is (NP_a_[\\w]+ ? (, )?(and |or )?)+)\", \"second\"),\n",
        "            (\"(NP_[\\w\\-]+ of (NP_[\\w\\-]+) (, )?includes (NP_[\\w\\-]+ ? (, )?(and |or )?)+)\", \"third\"),\n",
        "            (\"(there is NP_[\\w\\-]+ (, )?possibly (NP_[\\w\\-]+ ? (, )?(and |or )?)+)\", \"first\")\n",
        "        ]\n",
        "\n",
        "\n",
        "    if extended:\n",
        "      self.__hearst_patterns.extend([\n",
        "            (\"(NP_\\w+ (, )?such as (NP_\\w+ ? (, )?(and |or )?)+)\", \"first\"),\n",
        "            ])\n",
        "\n",
        "    self.__pos_tagger = PerceptronTagger()\n",
        "\n",
        "  def getPatterns(self):\n",
        "    return self.__hearst_patterns\n",
        "    \n",
        "  def prepare(self, rawtext):\n",
        "    # To process text in NLTK format\n",
        "    sentences = nltk.sent_tokenize(rawtext.strip())\n",
        "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "    sentences = [self.__pos_tagger.tag(sent) for sent in sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "  def chunk(self, rawtext):\n",
        "    sentences = self.prepare(rawtext.strip())\n",
        "\n",
        "    all_chunks = []\n",
        "    for sentence in sentences:\n",
        "      chunks = self.__np_chunker.parse(sentence)\n",
        "      all_chunks.append(self.prepare_chunks(chunks))\n",
        "\n",
        "    # two or more NPs next to each other should be merged into a single NP,\n",
        "    # find any N consecutive NP_ and merge them into one...\n",
        "    # Eg: \"NP_foo NP_bar blah blah\" becomes \"NP_foo_bar blah blah\"\n",
        "    all_sentences = []\n",
        "    for raw_sentence in all_chunks:\n",
        "      sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",\n",
        "                        lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),\n",
        "                        raw_sentence)\n",
        "      all_sentences.append(sentence)\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "  def prepare_chunks(self, chunks):\n",
        "    # If chunk is NP, start with NP_ and join tokens in chunk with _\n",
        "    # Else just keep the token as it is\n",
        "\n",
        "    terms = []\n",
        "    for chunk in chunks:\n",
        "      label = None\n",
        "      try:\n",
        "        # gross hack to see if the chunk is simply a word or a NP, as\n",
        "        # we want. But non-NP fail on this method call\n",
        "        label = chunk.label()\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      if label is None:  # means one word...\n",
        "        token = chunk[0]\n",
        "        terms.append(token)\n",
        "      else:\n",
        "        np = \"NP_\"+\"_\".join([a[0] for a in chunk])\n",
        "        terms.append(np)\n",
        "    \n",
        "    return ' '.join(terms)\n",
        "\n",
        "  def find_hyponyms(self, rawtext):\n",
        "    \n",
        "    hypo_hypernyms = []\n",
        "    np_tagged_sentences = self.chunk(rawtext)\n",
        "    \n",
        "    for sentence in np_tagged_sentences:\n",
        "      print(sentence)\n",
        "      for (hearst_pattern, parser) in self.__hearst_patterns:\n",
        "        matches = re.search(hearst_pattern, sentence)\n",
        "        if matches:\n",
        "          match_str = matches.group(0)\n",
        "          nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
        "          if parser == \"first\":\n",
        "            hypernym = nps[0]\n",
        "            hyponyms = nps[1:]\n",
        "          elif parser == \"third\":\n",
        "            hypernym = nps[0]\n",
        "            hyponyms = nps[2:]\n",
        "          else: # usually \"second\" \n",
        "            hypernym = nps[-1]\n",
        "            hyponyms = nps[:-1]\n",
        "            \n",
        "          for i in range(len(hyponyms)):\n",
        "            hypo_hypernyms.append(\n",
        "                (self.clean_hyponym_term(hyponyms[i]),\n",
        "                 self.clean_hyponym_term(hypernym)))\n",
        "\n",
        "    return hypo_hypernyms\n",
        "\n",
        "  def clean_hyponym_term(self, term):\n",
        "    term = term.replace(\"NP_\", \"\").replace(\"_\", \" \").split()[-1]\n",
        "    return term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W3pfXRZNKY-",
        "colab_type": "code",
        "outputId": "e5b6f983-b5a7-46e7-c4a9-166ddeac17eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "  \n",
        "  hp = HearstPatterns(extended = False)\n",
        "\n",
        "  # text = 'All musical genres, including blues and jazz.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "  \n",
        "  # text = 'I am going to get green vegetables such as spinach, peas and kale.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'I like to listen to music from musical genres such as blues and jazz.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'works by such authors as Herrick, Goldsmith, and Shakespeare.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'Bruises, wounds, broken bones or other injuries.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'temples, treasuries, and other important civic buildings'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'All common-law countries, including Canada and England.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'All musical genres, including blues and jazz.'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'most European countries, including Canada and England ...'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'For many years , the tavern has been a private residence and ironically , the principal mill has been a fine restaurant and bar .'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'The shelters are otherwise known as motorcycle sheds , bike sheds and motorbike sheds .\t'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'Carsluith Castle is a ruinous tower house , dating largely to the 16th century .'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'Typical wildlife of the North River includes Great Blue Heron , Wood Duck , Canada Goose , Belted Kingfisher , Baltimore Oriole , Painted Turtle , Common Snapping Turtle , Largemouth Bass , Sun Perch , Catfish , Eastern Cottontail Rabbit , White-tailed Deer , Raccoon , Opossum , Brown Bats , Freshwater Clams , Mink , Tiger Swallowtail and Ebony Jewelwing .'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  # text = 'Typical wildlife of the North River includes Great Blue Heron , Wood Duck , Canada Goose , Belted Kingfisher , Baltimore Oriole , Painted Turtle , Common Snapping Turtle , Largemouth Bass , Sun Perch , Catfish , Eastern Cottontail Rabbit , White-tailed Deer , Raccoon , Opossum , Brown Bats , Freshwater Clams , Mink , Tiger Swallowtail and Ebony Jewelwing .'\n",
        "  # hps = hp.find_hyponyms(text)\n",
        "  # print(hps)\n",
        "\n",
        "  text = 'Along the lower right side of the shaft , there is a small animal , possibly a cat .'\n",
        "  hps = hp.find_hyponyms(text)\n",
        "  print(hps)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Along NP_the_lower_right_side of NP_the_shaft , there is NP_a_small_animal , possibly NP_a_cat .\n",
            "[('cat', 'animal')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}