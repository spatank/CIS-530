{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "homework_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%205/homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS80Ll4dvIcX",
        "colab_type": "code",
        "outputId": "eaa93a17-e383-4d5c-fd73-3a2ed4280b88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "pip install pymagnitude"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymagnitude\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/a3/b9a34d22ed8c0ed59b00ff55092129641cdfa09d82f9abdc5088051a5b0c/pymagnitude-0.1.120.tar.gz (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 3.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n",
            "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymagnitude: filename=pymagnitude-0.1.120-cp36-cp36m-linux_x86_64.whl size=135918206 sha256=a9d3150c5cd7c1b0b3ad6552547aee29602a446c8be303a11387b695178dbfee\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/c7/98/cb48b9db35f8d1a7827b764dc36c5515179dc116448a47c8a1\n",
            "Successfully built pymagnitude\n",
            "Installing collected packages: pymagnitude\n",
            "Successfully installed pymagnitude-0.1.120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8WGugLYvUjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymagnitude import *\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "from itertools import combinations\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "import random\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFuioNiqbjQp",
        "colab_type": "code",
        "outputId": "fc888ee1-7293-42fb-d305-3d325b49f6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH6_3yAPlWgR",
        "colab_type": "code",
        "outputId": "513c8e11-d908-4c10-d7eb-bc03e460fbba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# to view contents, run following line\n",
        "# !ls drive/My\\ Drive/CIS-530/Homework\\ 5/Data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvKJrWiuPuZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_input_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the input file to two dictionaries\n",
        "    :param file_path: path to an input file\n",
        "    :return: 2 dictionaries:\n",
        "    1. Dictionary, where key is a target word and value is a list of paraphrases\n",
        "    2. Dictionary, where key is a target word and value is a number of clusters\n",
        "    \"\"\"\n",
        "    word_to_paraphrases_dict = {}\n",
        "    word_to_k_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, k, paraphrases = line.split(' :: ')\n",
        "            word_to_k_dict[target_word] = int(k)\n",
        "            word_to_paraphrases_dict[target_word] = paraphrases.split()\n",
        "\n",
        "    return word_to_paraphrases_dict, word_to_k_dict\n",
        "\n",
        "\n",
        "def load_output_file(file_path):\n",
        "    \"\"\"\n",
        "    :param file_path: path to an output file\n",
        "    :return: A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, _, paraphrases_in_cluster = line.strip().split(' :: ')\n",
        "            paraphrases_list = paraphrases_in_cluster.strip().split()\n",
        "            if target_word not in clusterings:\n",
        "                clusterings[target_word] = []\n",
        "            clusterings[target_word].append(paraphrases_list)\n",
        "\n",
        "    return clusterings\n",
        "\n",
        "\n",
        "def write_to_output_file(file_path, clusterings):\n",
        "    \"\"\"\n",
        "    Writes the result of clusterings into an output file\n",
        "    :param file_path: path to an output file\n",
        "    :param clusterings:  A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as fout:\n",
        "        for target_word, clustering in clusterings.items():\n",
        "            for i, cluster in enumerate(clustering):\n",
        "                fout.write(f'{target_word} :: {i + 1} :: {\" \".join(cluster)}\\n')\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "def get_paired_f_score(gold_clustering, predicted_clustering):\n",
        "    \"\"\"\n",
        "    :param gold_clustering: gold list of list of paraphrases\n",
        "    :param predicted_clustering: predicted list of list of paraphrases\n",
        "    :return: Paired F-Score\n",
        "    \"\"\"\n",
        "    gold_pairs = set()\n",
        "    for gold_cluster in gold_clustering:\n",
        "        for pair in combinations(gold_cluster, 2):\n",
        "            gold_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    predicted_pairs = set()\n",
        "    for predicted_cluster in predicted_clustering:\n",
        "        for pair in combinations(predicted_cluster, 2):\n",
        "            predicted_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    overlapping_pairs = gold_pairs & predicted_pairs\n",
        "\n",
        "    precision = 1. if len(predicted_pairs) == 0 else float(len(overlapping_pairs)) / len(predicted_pairs)\n",
        "    recall = 1. if len(gold_pairs) == 0 else float(len(overlapping_pairs)) / len(gold_pairs)\n",
        "    paired_f_score = 0. if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return paired_f_score\n",
        "\n",
        "\n",
        "def evaluate_clusterings(gold_clusterings, predicted_clusterings):\n",
        "    \"\"\"\n",
        "    Displays evaluation scores between gold and predicted clusterings\n",
        "    :param gold_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :param predicted_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    target_words = set(gold_clusterings.keys()) & set(predicted_clusterings.keys())\n",
        "\n",
        "    if len(target_words) == 0:\n",
        "        print('No overlapping target words in ground-truth and predicted files')\n",
        "        return None\n",
        "\n",
        "    paired_f_scores = np.zeros((len(target_words)))\n",
        "    ks = np.zeros((len(target_words)))\n",
        "\n",
        "    table = PrettyTable(['Target', 'k', 'Paired F-Score'])\n",
        "    for i, target_word in enumerate(target_words):\n",
        "        paired_f_score = get_paired_f_score(gold_clusterings[target_word], predicted_clusterings[target_word])\n",
        "        k = len(gold_clusterings[target_word])\n",
        "        paired_f_scores[i] = paired_f_score\n",
        "        ks[i] = k\n",
        "        table.add_row([target_word, k, f'{paired_f_score:0.4f}'])\n",
        "\n",
        "    average_f_score = np.average(paired_f_scores, weights=ks)\n",
        "    print(table)\n",
        "    print(f'=> Average Paired F-Score:  {average_f_score:.4f}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO8PtCMkzP73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(123)\n",
        "\n",
        "# TASK 3.1\n",
        "def cluster_random(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases randomly\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word] # number of clusters for target word\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be randomly chosen\n",
        "        for cluster in range(k): \n",
        "          # each word must have a cluster, each cluster must have a word\n",
        "          cluster_list = random.choices(paraphrase_list, k = int(np.round(len(paraphrase_list)/k)))\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QcL2uySlmGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "# output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "# gold_clusterings = load_output_file(output_filepath)\n",
        "# predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ81Z6GzxzhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_random.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndW7rqYsWhDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_syllables(word): \n",
        "\n",
        "    word = word.lower() \n",
        "    # exception_add are words that need extra syllables\n",
        "    # exception_del are words that need less syllables\n",
        "    exception_add = ['serious','crucial']\n",
        "    exception_del = ['fortunately','unfortunately'] \n",
        "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
        "    co_two = ['coapt','coed','coinci']\n",
        "    pre_one = ['preach']\n",
        " \n",
        "    syls = 0 # added syllable number\n",
        "    disc = 0 # discarded syllable number\n",
        " \n",
        "    #1) if letters < 3 : return 1\n",
        "    if len(word) <= 3 :\n",
        "        syls = 1\n",
        "        return syls\n",
        " \n",
        "    #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
        "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.) \n",
        "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
        "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
        "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
        "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
        "                pass\n",
        "            else :\n",
        "                disc += 1\n",
        " \n",
        "    #3) discard trailing \"e\", except where ending is \"le\"   \n",
        "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while'] \n",
        "    if word[-1:] == \"e\" :\n",
        "        if word[-2:] == \"le\" and word not in le_except :\n",
        "            pass \n",
        "        else :\n",
        "            disc += 1\n",
        " \n",
        "    #4) check if consecutive vowels exists, triplets or pairs, count them as one. \n",
        "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
        "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
        "    disc += doubleAndtripple + tripple\n",
        " \n",
        "    # 5) count remaining vowels in word.\n",
        "    numVowels = len(re.findall(r'[eaoui]',word))\n",
        " \n",
        "    # 6) add one if starts with \"mc\"\n",
        "    if word[:2] == \"mc\" :\n",
        "        syls+=1\n",
        " \n",
        "    # 7) add one if ends with \"y\" but is not surrouned by vowel\n",
        "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
        "        syls +=1\n",
        " \n",
        "    # 8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
        "    for i,j in enumerate(word) :\n",
        "        if j == \"y\" :\n",
        "            if (i != 0) and (i != len(word)-1) :\n",
        "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
        "                    syls+=1\n",
        " \n",
        "    # 9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one. \n",
        "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
        "        syls+=1 \n",
        "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
        "        syls+=1\n",
        " \n",
        "    # 10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
        "    if word[-3:] == \"ian\" : \n",
        "    # and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
        "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, \n",
        "    #     if not, check if in single dictionary and act accordingly. \n",
        "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
        "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
        "            syls+=1\n",
        "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, \n",
        "    #     if not, check if in single dictionary and act accordingly. \n",
        "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
        "        if word[:6] in pre_one :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
        "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"] \n",
        "    if word[-3:] == \"n't\" :\n",
        "        if word in negative :\n",
        "            syls+=1\n",
        "        else :\n",
        "            pass  \n",
        " \n",
        "    # 14) Handling the exceptional words. \n",
        "    if word in exception_del :\n",
        "        disc+=1 \n",
        "    if word in exception_add :\n",
        "        syls+=1    \n",
        " \n",
        "    # calculate the output\n",
        "    return numVowels - disc + syls\n",
        "\n",
        "\n",
        "def load_ngram_counts(ngram_counts_file): \n",
        "   counts = defaultdict(int) \n",
        "   with gzip.open(ngram_counts_file, 'rt') as f: \n",
        "       for line in f:\n",
        "           token, count = line.strip().split('\\t') \n",
        "           if token[0].islower(): \n",
        "               counts[token] = int(count) \n",
        "   return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adg1ux23lIU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(paraphrase):\n",
        "  features_dict = {'pos': nltk.pos_tag(paraphrase)[0][1],\n",
        "                   'syl': count_syllables(paraphrase)}\n",
        "  return features_dict\n",
        "\n",
        "\n",
        "# TASK 3.2\n",
        "def cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'coocvec-500mostfreq-window-3.filter.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = False)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word]\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X_vector = np.zeros((len(paraphrase_list), 500))\n",
        "        # X_features = []\n",
        "        # build data matrix\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          if paraphrase not in vectors:\n",
        "            # print(paraphrase)\n",
        "            X_vector[idx, :] = random.choice(vectors)[1]\n",
        "          X_vector[idx, :] = vectors.query(paraphrase)\n",
        "          # features_dict = extract_features(paraphrase)\n",
        "          # X_features.append(features_dict)\n",
        "        vectorizer = DictVectorizer(sparse = False)\n",
        "\n",
        "        # X_features = vectorizer.fit_transform(X_features)\n",
        "\n",
        "        # X = np.concatenate((X_vector, X_features), axis = 1)\n",
        "        X = X_vector\n",
        "\n",
        "        # kmeans = KMeans(n_clusters = k).fit(X)\n",
        "        AC = AgglomerativeClustering(n_clusters = k, affinity = 'manhattan', linkage = 'single').fit(X)\n",
        "\n",
        "        for cluster in range(k):\n",
        "          # cluster_list = [paraphrase_list[idx] \n",
        "          #                 for idx, label in enumerate(kmeans.labels_) \n",
        "          #                 if label == cluster]\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(AC.labels_) \n",
        "                          if label == cluster]\n",
        "          if len(cluster_list) == 0:\n",
        "            print('Empty cluster\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TxsDraa0Vcd",
        "colab_type": "code",
        "outputId": "c2e3a201-4ff0-49f2-f0ca-9e841e236ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    image.n     | 9  |     0.3332     |\n",
            "|  difference.n  | 5  |     0.5337     |\n",
            "|   shelter.n    | 5  |     0.5032     |\n",
            "|    climb.v     | 6  |     0.3375     |\n",
            "|     use.v      | 6  |     0.6461     |\n",
            "|   judgment.n   | 7  |     0.3296     |\n",
            "|   suspend.v    | 6  |     0.2453     |\n",
            "|    smell.v     | 4  |     0.5484     |\n",
            "|   produce.v    | 7  |     0.4698     |\n",
            "|     win.v      | 4  |     0.5247     |\n",
            "|     eat.v      | 6  |     0.4341     |\n",
            "| organization.n | 7  |     0.3917     |\n",
            "|     mean.v     | 6  |     0.3714     |\n",
            "|  atmosphere.n  | 6  |     0.4203     |\n",
            "|     plan.n     | 3  |     0.6387     |\n",
            "|     note.v     | 3  |     0.6400     |\n",
            "|    write.v     | 9  |     0.3669     |\n",
            "|     miss.v     | 8  |     0.3333     |\n",
            "| performance.n  | 5  |     0.4840     |\n",
            "|    degree.n    | 7  |     0.4182     |\n",
            "|     rule.v     | 7  |     0.3481     |\n",
            "|   provide.v    | 7  |     0.7374     |\n",
            "|    party.n     | 5  |     0.3882     |\n",
            "|     talk.v     | 6  |     0.6338     |\n",
            "|   express.v    | 7  |     0.4187     |\n",
            "|     hear.v     | 5  |     0.3319     |\n",
            "|    expect.v    | 6  |     0.4924     |\n",
            "|    paper.n     | 7  |     0.6747     |\n",
            "|    source.n    | 9  |     0.2978     |\n",
            "|     play.v     | 34 |     0.1904     |\n",
            "|    watch.v     | 5  |     0.3300     |\n",
            "|    treat.v     | 8  |     0.4077     |\n",
            "|     wash.v     | 13 |     0.3602     |\n",
            "|    simple.a    | 5  |     0.3590     |\n",
            "|    begin.v     | 8  |     0.2892     |\n",
            "|   interest.n   | 5  |     0.3121     |\n",
            "|   operate.v    | 7  |     0.2844     |\n",
            "|   receive.v    | 13 |     0.1357     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|     bank.n     | 9  |     0.3000     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7tFR-iUHjjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_sparse.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwZ0aHcjlLO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 3.3\n",
        "def cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'GoogleNews-vectors-negative300.filter.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = True)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word]\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X_vector = np.zeros((len(paraphrase_list), 300))\n",
        "        # X_features = []\n",
        "        # build data matrix\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          # if paraphrase not in vectors:\n",
        "          #   # print(paraphrase)\n",
        "          #   X_vector[idx, :] = random.choice(vectors)[1]\n",
        "          X_vector[idx, :] = vectors.query(paraphrase)\n",
        "          # features_dict = extract_features(paraphrase)\n",
        "          # X_features.append(features_dict)\n",
        "        vectorizer = DictVectorizer(sparse = False)\n",
        "\n",
        "        # X_features = vectorizer.fit_transform(X_features)\n",
        "\n",
        "        # X = np.concatenate((X_vector, X_features), axis = 1)\n",
        "        X = X_vector\n",
        "\n",
        "        # kmeans = KMeans(n_clusters = k).fit(X)\n",
        "        AC = AgglomerativeClustering(n_clusters = k, affinity = 'manhattan', linkage = 'single').fit(X)\n",
        "\n",
        "        for cluster in range(k):\n",
        "          # cluster_list = [paraphrase_list[idx] \n",
        "          #                 for idx, label in enumerate(kmeans.labels_) \n",
        "          #                 if label == cluster]\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(AC.labels_) \n",
        "                          if label == cluster]\n",
        "          if len(cluster_list) == 0:\n",
        "            print('Empty cluster\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya-CeOl1tHFm",
        "colab_type": "code",
        "outputId": "3298cd2a-0b27-4503-8ca8-1804912d1bb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    image.n     | 9  |     0.3278     |\n",
            "|  difference.n  | 5  |     0.5107     |\n",
            "|   shelter.n    | 5  |     0.5821     |\n",
            "|    climb.v     | 6  |     0.3176     |\n",
            "|     use.v      | 6  |     0.6316     |\n",
            "|   judgment.n   | 7  |     0.3336     |\n",
            "|   suspend.v    | 6  |     0.3256     |\n",
            "|    smell.v     | 4  |     0.4677     |\n",
            "|   produce.v    | 7  |     0.4548     |\n",
            "|     win.v      | 4  |     0.4892     |\n",
            "|     eat.v      | 6  |     0.4614     |\n",
            "| organization.n | 7  |     0.3904     |\n",
            "|     mean.v     | 6  |     0.3646     |\n",
            "|  atmosphere.n  | 6  |     0.4709     |\n",
            "|     plan.n     | 3  |     0.7261     |\n",
            "|     note.v     | 3  |     0.8400     |\n",
            "|    write.v     | 9  |     0.3400     |\n",
            "|     miss.v     | 8  |     0.3964     |\n",
            "| performance.n  | 5  |     0.4718     |\n",
            "|    degree.n    | 7  |     0.4315     |\n",
            "|     rule.v     | 7  |     0.3165     |\n",
            "|   provide.v    | 7  |     0.7139     |\n",
            "|    party.n     | 5  |     0.4323     |\n",
            "|     talk.v     | 6  |     0.6504     |\n",
            "|   express.v    | 7  |     0.4206     |\n",
            "|     hear.v     | 5  |     0.3789     |\n",
            "|    expect.v    | 6  |     0.5079     |\n",
            "|    paper.n     | 7  |     0.5633     |\n",
            "|    source.n    | 9  |     0.3534     |\n",
            "|     play.v     | 34 |     0.1566     |\n",
            "|    watch.v     | 5  |     0.5300     |\n",
            "|    treat.v     | 8  |     0.3903     |\n",
            "|     wash.v     | 13 |     0.3403     |\n",
            "|    simple.a    | 5  |     0.3448     |\n",
            "|    begin.v     | 8  |     0.3971     |\n",
            "|   interest.n   | 5  |     0.3716     |\n",
            "|   operate.v    | 7  |     0.3124     |\n",
            "|   receive.v    | 13 |     0.2141     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|     bank.n     | 9  |     0.6154     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzaBCLlNtRvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_dense.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ZAYAt9lN-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 3.4\n",
        "def cluster_with_no_k(word_to_paraphrases_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using any vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.filter.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        # TODO: Implement\n",
        "        clusterings[target_word] = None\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-Jc6--YPpXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_no_k(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urEw5dghN6h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_paraphrases_dict, _ = load_input_file('data/test_nok_input.txt')\n",
        "predicted_clusterings = cluster_with_no_k(word_to_paraphrases_dict)\n",
        "write_to_output_file('test_output_nok.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}