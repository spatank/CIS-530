{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "homework_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/Homework%205/homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS80Ll4dvIcX",
        "colab_type": "code",
        "outputId": "74bbabf3-b10c-4d7e-ba38-cdfa0b462d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "pip install pymagnitude"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymagnitude\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/a3/b9a34d22ed8c0ed59b00ff55092129641cdfa09d82f9abdc5088051a5b0c/pymagnitude-0.1.120.tar.gz (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n",
            "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymagnitude: filename=pymagnitude-0.1.120-cp36-cp36m-linux_x86_64.whl size=135918206 sha256=ac49e59f18d587a03adc3589a96cb21d83985ee1a077a476961a51e26dfa5fdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/c7/98/cb48b9db35f8d1a7827b764dc36c5515179dc116448a47c8a1\n",
            "Successfully built pymagnitude\n",
            "Installing collected packages: pymagnitude\n",
            "Successfully installed pymagnitude-0.1.120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8WGugLYvUjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymagnitude import *\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "from itertools import combinations\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "import random\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFuioNiqbjQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e6149ac6-8270-44dc-de95-ecfa8417501c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH6_3yAPlWgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6d32b786-b2ac-461c-b69d-a460c37eb4fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# to view contents, run following line\n",
        "# !ls drive/My\\ Drive/CIS-530/Homework\\ 5/Data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvKJrWiuPuZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_input_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the input file to two dictionaries\n",
        "    :param file_path: path to an input file\n",
        "    :return: 2 dictionaries:\n",
        "    1. Dictionary, where key is a target word and value is a list of paraphrases\n",
        "    2. Dictionary, where key is a target word and value is a number of clusters\n",
        "    \"\"\"\n",
        "    word_to_paraphrases_dict = {}\n",
        "    word_to_k_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, k, paraphrases = line.split(' :: ')\n",
        "            word_to_k_dict[target_word] = int(k)\n",
        "            word_to_paraphrases_dict[target_word] = paraphrases.split()\n",
        "\n",
        "    return word_to_paraphrases_dict, word_to_k_dict\n",
        "\n",
        "\n",
        "def load_output_file(file_path):\n",
        "    \"\"\"\n",
        "    :param file_path: path to an output file\n",
        "    :return: A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, _, paraphrases_in_cluster = line.strip().split(' :: ')\n",
        "            paraphrases_list = paraphrases_in_cluster.strip().split()\n",
        "            if target_word not in clusterings:\n",
        "                clusterings[target_word] = []\n",
        "            clusterings[target_word].append(paraphrases_list)\n",
        "\n",
        "    return clusterings\n",
        "\n",
        "\n",
        "def write_to_output_file(file_path, clusterings):\n",
        "    \"\"\"\n",
        "    Writes the result of clusterings into an output file\n",
        "    :param file_path: path to an output file\n",
        "    :param clusterings:  A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as fout:\n",
        "        for target_word, clustering in clusterings.items():\n",
        "            for i, cluster in enumerate(clustering):\n",
        "                fout.write(f'{target_word} :: {i + 1} :: {\" \".join(cluster)}\\n')\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "def get_paired_f_score(gold_clustering, predicted_clustering):\n",
        "    \"\"\"\n",
        "    :param gold_clustering: gold list of list of paraphrases\n",
        "    :param predicted_clustering: predicted list of list of paraphrases\n",
        "    :return: Paired F-Score\n",
        "    \"\"\"\n",
        "    gold_pairs = set()\n",
        "    for gold_cluster in gold_clustering:\n",
        "        for pair in combinations(gold_cluster, 2):\n",
        "            gold_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    predicted_pairs = set()\n",
        "    for predicted_cluster in predicted_clustering:\n",
        "        for pair in combinations(predicted_cluster, 2):\n",
        "            predicted_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    overlapping_pairs = gold_pairs & predicted_pairs\n",
        "\n",
        "    precision = 1. if len(predicted_pairs) == 0 else float(len(overlapping_pairs)) / len(predicted_pairs)\n",
        "    recall = 1. if len(gold_pairs) == 0 else float(len(overlapping_pairs)) / len(gold_pairs)\n",
        "    paired_f_score = 0. if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return paired_f_score\n",
        "\n",
        "\n",
        "def evaluate_clusterings(gold_clusterings, predicted_clusterings):\n",
        "    \"\"\"\n",
        "    Displays evaluation scores between gold and predicted clusterings\n",
        "    :param gold_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :param predicted_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    target_words = set(gold_clusterings.keys()) & set(predicted_clusterings.keys())\n",
        "\n",
        "    if len(target_words) == 0:\n",
        "        print('No overlapping target words in ground-truth and predicted files')\n",
        "        return None\n",
        "\n",
        "    paired_f_scores = np.zeros((len(target_words)))\n",
        "    ks = np.zeros((len(target_words)))\n",
        "\n",
        "    table = PrettyTable(['Target', 'k', 'Paired F-Score'])\n",
        "    for i, target_word in enumerate(target_words):\n",
        "        paired_f_score = get_paired_f_score(gold_clusterings[target_word], predicted_clusterings[target_word])\n",
        "        k = len(gold_clusterings[target_word])\n",
        "        paired_f_scores[i] = paired_f_score\n",
        "        ks[i] = k\n",
        "        table.add_row([target_word, k, f'{paired_f_score:0.4f}'])\n",
        "\n",
        "    average_f_score = np.average(paired_f_scores, weights=ks)\n",
        "    print(table)\n",
        "    print(f'=> Average Paired F-Score:  {average_f_score:.4f}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO8PtCMkzP73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(123)\n",
        "\n",
        "# TASK 3.1\n",
        "def cluster_random(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases randomly\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word] # number of clusters for target word\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be randomly chosen\n",
        "        for cluster in range(k): \n",
        "          # each word must have a cluster, each cluster must have a word\n",
        "          cluster_list = random.choices(paraphrase_list, k = int(np.round(len(paraphrase_list)/k)))\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QcL2uySlmGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "# output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "# gold_clusterings = load_output_file(output_filepath)\n",
        "# predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ81Z6GzxzhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_random.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndW7rqYsWhDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_syllables(word): \n",
        "\n",
        "    word = word.lower() \n",
        "    # exception_add are words that need extra syllables\n",
        "    # exception_del are words that need less syllables\n",
        "    exception_add = ['serious','crucial']\n",
        "    exception_del = ['fortunately','unfortunately'] \n",
        "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
        "    co_two = ['coapt','coed','coinci']\n",
        "    pre_one = ['preach']\n",
        " \n",
        "    syls = 0 # added syllable number\n",
        "    disc = 0 # discarded syllable number\n",
        " \n",
        "    #1) if letters < 3 : return 1\n",
        "    if len(word) <= 3 :\n",
        "        syls = 1\n",
        "        return syls\n",
        " \n",
        "    #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
        "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.) \n",
        "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
        "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
        "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
        "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
        "                pass\n",
        "            else :\n",
        "                disc += 1\n",
        " \n",
        "    #3) discard trailing \"e\", except where ending is \"le\"   \n",
        "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while'] \n",
        "    if word[-1:] == \"e\" :\n",
        "        if word[-2:] == \"le\" and word not in le_except :\n",
        "            pass \n",
        "        else :\n",
        "            disc += 1\n",
        " \n",
        "    #4) check if consecutive vowels exists, triplets or pairs, count them as one. \n",
        "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
        "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
        "    disc += doubleAndtripple + tripple\n",
        " \n",
        "    # 5) count remaining vowels in word.\n",
        "    numVowels = len(re.findall(r'[eaoui]',word))\n",
        " \n",
        "    # 6) add one if starts with \"mc\"\n",
        "    if word[:2] == \"mc\" :\n",
        "        syls+=1\n",
        " \n",
        "    # 7) add one if ends with \"y\" but is not surrouned by vowel\n",
        "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
        "        syls +=1\n",
        " \n",
        "    # 8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
        "    for i,j in enumerate(word) :\n",
        "        if j == \"y\" :\n",
        "            if (i != 0) and (i != len(word)-1) :\n",
        "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
        "                    syls+=1\n",
        " \n",
        "    # 9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one. \n",
        "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
        "        syls+=1 \n",
        "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
        "        syls+=1\n",
        " \n",
        "    # 10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
        "    if word[-3:] == \"ian\" : \n",
        "    # and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
        "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, \n",
        "    #     if not, check if in single dictionary and act accordingly. \n",
        "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
        "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
        "            syls+=1\n",
        "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, \n",
        "    #     if not, check if in single dictionary and act accordingly. \n",
        "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
        "        if word[:6] in pre_one :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
        "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"] \n",
        "    if word[-3:] == \"n't\" :\n",
        "        if word in negative :\n",
        "            syls+=1\n",
        "        else :\n",
        "            pass  \n",
        " \n",
        "    # 14) Handling the exceptional words. \n",
        "    if word in exception_del :\n",
        "        disc+=1 \n",
        "    if word in exception_add :\n",
        "        syls+=1    \n",
        " \n",
        "    # calculate the output\n",
        "    return numVowels - disc + syls\n",
        "\n",
        "\n",
        "def load_ngram_counts(ngram_counts_file): \n",
        "   counts = defaultdict(int) \n",
        "   with gzip.open(ngram_counts_file, 'rt') as f: \n",
        "       for line in f:\n",
        "           token, count = line.strip().split('\\t') \n",
        "           if token[0].islower(): \n",
        "               counts[token] = int(count) \n",
        "   return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adg1ux23lIU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # TASK 3.2\n",
        "# def cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "#     \"\"\"\n",
        "#     Clusters paraphrases using sparse vector representation\n",
        "#     :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "#     :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "#     :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "#     where each list corresponds to a cluster\n",
        "#     \"\"\"\n",
        "#     # Note: any vector representation should be in the same directory as this file\n",
        "#     vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "#     vectors_path = 'coocvec-500mostfreq-window-3.filter.magnitude'\n",
        "#     vectors = Magnitude(vectors_root_path + vectors_path, normalized = False)\n",
        "\n",
        "#     # ngram_counts_file = 'drive/My Drive/CIS-530/Homework 5/ngram_counts.txt.gz'\n",
        "#     # counts = load_ngram_counts(ngram_counts_file)\n",
        "\n",
        "#     clusterings = {}\n",
        "\n",
        "#     for target_word in word_to_paraphrases_dict.keys():\n",
        "#         paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "#         clusters = []\n",
        "#         k = word_to_k_dict[target_word]\n",
        "#         chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "#         X = np.zeros((len(paraphrase_list), 500))\n",
        "#         # build data matrix\n",
        "#         for idx, paraphrase in enumerate(paraphrase_list):\n",
        "#           X[idx,:] = vectors.query(paraphrase)\n",
        "#         kmeans = KMeans(n_clusters = k).fit(X)\n",
        "#         for cluster in range(k):\n",
        "#           cluster_list = [paraphrase_list[idx] \n",
        "#                           for idx, label in enumerate(kmeans.labels_) \n",
        "#                           if label == cluster]\n",
        "#           # if len(cluster_list) == 0:\n",
        "#           #   print('Empty cluster\\n')\n",
        "#           chosen_paraphrases.update(cluster_list)\n",
        "#           clusters.append(cluster_list)\n",
        "#         for paraphrase in paraphrase_list:\n",
        "#           if paraphrase not in chosen_paraphrases:\n",
        "#             # choose a random cluster list and append unassigned word to it\n",
        "#             random.choice(clusters).append(paraphrase) \n",
        "#         clusterings[target_word] = clusters\n",
        "\n",
        "#     return clusterings\n",
        "\n",
        "def extract_features(paraphrase):\n",
        "  features_dict = {'pos': nltk.pos_tag(paraphrase)[0][1],\n",
        "                   'syl': count_syllables(paraphrase)}\n",
        "  return features_dict\n",
        "\n",
        "\n",
        "# TASK 3.2\n",
        "def cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'coocvec-500mostfreq-window-3.filter.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = False)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word]\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X_vector = np.zeros((len(paraphrase_list), 500))\n",
        "        X_features = []\n",
        "        # build data matrix\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          X_vector[idx, :] = vectors.query(paraphrase)\n",
        "          features_dict = extract_features(paraphrase)\n",
        "          X_features.append(features_dict)\n",
        "        vectorizer = DictVectorizer(sparse = False)\n",
        "        X_features = vectorizer.fit_transform(X_features)\n",
        "\n",
        "        X = np.concatenate((X_vector, X_features), axis = 1)\n",
        "\n",
        "        kmeans = KMeans(n_clusters = k).fit(X)\n",
        "        for cluster in range(k):\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(kmeans.labels_) \n",
        "                          if label == cluster]\n",
        "          # if len(cluster_list) == 0:\n",
        "          #   print('Empty cluster\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TxsDraa0Vcd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "c3404706-de76-4d8b-8034-006d98f1639b"
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    image.n     | 9  |     0.3174     |\n",
            "|   interest.n   | 5  |     0.2986     |\n",
            "|     plan.n     | 3  |     0.5809     |\n",
            "|     use.v      | 6  |     0.5915     |\n",
            "|     win.v      | 4  |     0.3970     |\n",
            "|    treat.v     | 8  |     0.4200     |\n",
            "|   suspend.v    | 6  |     0.2316     |\n",
            "|    degree.n    | 7  |     0.4240     |\n",
            "|  difference.n  | 5  |     0.4967     |\n",
            "|    climb.v     | 6  |     0.3247     |\n",
            "|     hear.v     | 5  |     0.3119     |\n",
            "|   shelter.n    | 5  |     0.5060     |\n",
            "|    simple.a    | 5  |     0.3590     |\n",
            "|  atmosphere.n  | 6  |     0.3339     |\n",
            "|   receive.v    | 13 |     0.0900     |\n",
            "|     rule.v     | 7  |     0.3577     |\n",
            "|   provide.v    | 7  |     0.6593     |\n",
            "| performance.n  | 5  |     0.4853     |\n",
            "|     talk.v     | 6  |     0.6221     |\n",
            "|     wash.v     | 13 |     0.3805     |\n",
            "|    watch.v     | 5  |     0.3243     |\n",
            "|     bank.n     | 9  |     0.3619     |\n",
            "|     miss.v     | 8  |     0.3577     |\n",
            "|     eat.v      | 6  |     0.4443     |\n",
            "|   operate.v    | 7  |     0.2881     |\n",
            "|    expect.v    | 6  |     0.5321     |\n",
            "|   produce.v    | 7  |     0.4390     |\n",
            "|     note.v     | 3  |     0.5238     |\n",
            "|    party.n     | 5  |     0.3882     |\n",
            "|   judgment.n   | 7  |     0.3128     |\n",
            "| organization.n | 7  |     0.3248     |\n",
            "|    source.n    | 9  |     0.3200     |\n",
            "|    begin.v     | 8  |     0.2576     |\n",
            "|   express.v    | 7  |     0.4425     |\n",
            "|     mean.v     | 6  |     0.3261     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|    paper.n     | 7  |     0.6747     |\n",
            "|    smell.v     | 4  |     0.5484     |\n",
            "|    write.v     | 9  |     0.3183     |\n",
            "|     play.v     | 34 |     0.1784     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7tFR-iUHjjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "predicted_clusterings = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_sparse.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwZ0aHcjlLO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 3.3\n",
        "def cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'GoogleNews-vectors-negative300.filter.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = False)\n",
        "\n",
        "    # ngram_counts_file = 'drive/My Drive/CIS-530/Homework 5/ngram_counts.txt.gz'\n",
        "    # counts = load_ngram_counts(ngram_counts_file)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word]\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X = np.zeros((len(paraphrase_list), 300))\n",
        "        # build data matrix\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          X[idx,:] = vectors.query(paraphrase)\n",
        "        kmeans = KMeans(n_clusters = k).fit(X)\n",
        "        for cluster in range(k):\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(kmeans.labels_) \n",
        "                          if label == cluster]\n",
        "          # if len(cluster_list) == 0:\n",
        "          #   print('Empty cluster\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya-CeOl1tHFm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "e4d2200d-f82c-4b16-8e29-b6014759e70a"
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    image.n     | 9  |     0.2535     |\n",
            "|   interest.n   | 5  |     0.3032     |\n",
            "|     plan.n     | 3  |     0.4455     |\n",
            "|     use.v      | 6  |     0.5259     |\n",
            "|     win.v      | 4  |     0.3892     |\n",
            "|    treat.v     | 8  |     0.3385     |\n",
            "|   suspend.v    | 6  |     0.3429     |\n",
            "|    degree.n    | 7  |     0.3451     |\n",
            "|  difference.n  | 5  |     0.3462     |\n",
            "|    climb.v     | 6  |     0.2871     |\n",
            "|     hear.v     | 5  |     0.3715     |\n",
            "|   shelter.n    | 5  |     0.3853     |\n",
            "|    simple.a    | 5  |     0.3030     |\n",
            "|  atmosphere.n  | 6  |     0.2910     |\n",
            "|   receive.v    | 13 |     0.2308     |\n",
            "|     rule.v     | 7  |     0.2687     |\n",
            "|   provide.v    | 7  |     0.4431     |\n",
            "| performance.n  | 5  |     0.4939     |\n",
            "|     talk.v     | 6  |     0.4409     |\n",
            "|     wash.v     | 13 |     0.2327     |\n",
            "|    watch.v     | 5  |     0.4757     |\n",
            "|     bank.n     | 9  |     0.3014     |\n",
            "|     miss.v     | 8  |     0.3833     |\n",
            "|     eat.v      | 6  |     0.3574     |\n",
            "|   operate.v    | 7  |     0.2993     |\n",
            "|    expect.v    | 6  |     0.3544     |\n",
            "|   produce.v    | 7  |     0.3279     |\n",
            "|     note.v     | 3  |     0.6400     |\n",
            "|    party.n     | 5  |     0.3165     |\n",
            "|   judgment.n   | 7  |     0.3359     |\n",
            "| organization.n | 7  |     0.3980     |\n",
            "|    source.n    | 9  |     0.2851     |\n",
            "|    begin.v     | 8  |     0.4324     |\n",
            "|   express.v    | 7  |     0.2453     |\n",
            "|     mean.v     | 6  |     0.3655     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|    paper.n     | 7  |     0.5267     |\n",
            "|    smell.v     | 4  |     0.2857     |\n",
            "|    write.v     | 9  |     0.3345     |\n",
            "|     play.v     | 34 |     0.1504     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzaBCLlNtRvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "predicted_clusterings = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_dense.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ZAYAt9lN-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 3.4\n",
        "def cluster_with_no_k(word_to_paraphrases_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using any vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors = Magnitude(\"GoogleNews-vectors-negative300.filter.magnitude\")\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        # TODO: Implement\n",
        "        clusterings[target_word] = None\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}