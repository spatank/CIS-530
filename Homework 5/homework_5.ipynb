{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "homework_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-530/blob/master/homework_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhTMBsdxtCkN",
        "colab_type": "code",
        "outputId": "2a102f94-b801-4e7d-e2f1-b6948e407303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "pip install pymagnitude"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymagnitude\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/a3/b9a34d22ed8c0ed59b00ff55092129641cdfa09d82f9abdc5088051a5b0c/pymagnitude-0.1.120.tar.gz (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 2.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n",
            "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymagnitude: filename=pymagnitude-0.1.120-cp36-cp36m-linux_x86_64.whl size=135918206 sha256=c8a992cd5fd0b1128bb6e25b2de67f0cbba61621593595da18b6784254db2c15\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/c7/98/cb48b9db35f8d1a7827b764dc36c5515179dc116448a47c8a1\n",
            "Successfully built pymagnitude\n",
            "Installing collected packages: pymagnitude\n",
            "Successfully installed pymagnitude-0.1.120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke3xs5D7tCkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymagnitude import *\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "from itertools import combinations\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUpEyFFZMkIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c2816c29-9f8b-4a5c-8b4f-63523aa8ae57"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYG9RGU8Mn2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e3ff4e9a-bb23-4fbc-d81a-a7e083a9ccc5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# to view contents, run following line\n",
        "# !ls drive/My\\ Drive/CIS-530/Homework\\ 5/Data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voXlp9H4Msl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_input_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads the input file to two dictionaries\n",
        "    :param file_path: path to an input file\n",
        "    :return: 2 dictionaries:\n",
        "    1. Dictionary, where key is a target word and value is a list of paraphrases\n",
        "    2. Dictionary, where key is a target word and value is a number of clusters\n",
        "    \"\"\"\n",
        "    word_to_paraphrases_dict = {}\n",
        "    word_to_k_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, k, paraphrases = line.split(' :: ')\n",
        "            word_to_k_dict[target_word] = int(k)\n",
        "            word_to_paraphrases_dict[target_word] = paraphrases.split()\n",
        "\n",
        "    return word_to_paraphrases_dict, word_to_k_dict\n",
        "\n",
        "\n",
        "def load_output_file(file_path):\n",
        "    \"\"\"\n",
        "    :param file_path: path to an output file\n",
        "    :return: A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    with open(file_path, 'r') as fin:\n",
        "        for line in fin:\n",
        "            target_word, _, paraphrases_in_cluster = line.strip().split(' :: ')\n",
        "            paraphrases_list = paraphrases_in_cluster.strip().split()\n",
        "            if target_word not in clusterings:\n",
        "                clusterings[target_word] = []\n",
        "            clusterings[target_word].append(paraphrases_list)\n",
        "\n",
        "    return clusterings\n",
        "\n",
        "\n",
        "def write_to_output_file(file_path, clusterings):\n",
        "    \"\"\"\n",
        "    Writes the result of clusterings into an output file\n",
        "    :param file_path: path to an output file\n",
        "    :param clusterings:  A dictionary, where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as fout:\n",
        "        for target_word, clustering in clusterings.items():\n",
        "            for i, cluster in enumerate(clustering):\n",
        "                fout.write(f'{target_word} :: {i + 1} :: {\" \".join(cluster)}\\n')\n",
        "        fout.close()\n",
        "\n",
        "\n",
        "def get_paired_f_score(gold_clustering, predicted_clustering):\n",
        "    \"\"\"\n",
        "    :param gold_clustering: gold list of list of paraphrases\n",
        "    :param predicted_clustering: predicted list of list of paraphrases\n",
        "    :return: Paired F-Score\n",
        "    \"\"\"\n",
        "    gold_pairs = set()\n",
        "    for gold_cluster in gold_clustering:\n",
        "        for pair in combinations(gold_cluster, 2):\n",
        "            gold_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    predicted_pairs = set()\n",
        "    for predicted_cluster in predicted_clustering:\n",
        "        for pair in combinations(predicted_cluster, 2):\n",
        "            predicted_pairs.add(tuple(sorted(pair)))\n",
        "\n",
        "    overlapping_pairs = gold_pairs & predicted_pairs\n",
        "\n",
        "    precision = 1. if len(predicted_pairs) == 0 else float(len(overlapping_pairs)) / len(predicted_pairs)\n",
        "    recall = 1. if len(gold_pairs) == 0 else float(len(overlapping_pairs)) / len(gold_pairs)\n",
        "    paired_f_score = 0. if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    return paired_f_score\n",
        "\n",
        "\n",
        "def evaluate_clusterings(gold_clusterings, predicted_clusterings):\n",
        "    \"\"\"\n",
        "    Displays evaluation scores between gold and predicted clusterings\n",
        "    :param gold_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :param predicted_clusterings: dictionary where key is a target word and value is a list of list of paraphrases\n",
        "    :return: N/A\n",
        "    \"\"\"\n",
        "    target_words = set(gold_clusterings.keys()) & set(predicted_clusterings.keys())\n",
        "\n",
        "    if len(target_words) == 0:\n",
        "        print('No overlapping target words in ground-truth and predicted files')\n",
        "        return None\n",
        "\n",
        "    paired_f_scores = np.zeros((len(target_words)))\n",
        "    ks = np.zeros((len(target_words)))\n",
        "\n",
        "    table = PrettyTable(['Target', 'k', 'Paired F-Score'])\n",
        "    for i, target_word in enumerate(target_words):\n",
        "        paired_f_score = get_paired_f_score(gold_clusterings[target_word], predicted_clusterings[target_word])\n",
        "        k = len(gold_clusterings[target_word])\n",
        "        paired_f_scores[i] = paired_f_score\n",
        "        ks[i] = k\n",
        "        table.add_row([target_word, k, f'{paired_f_score:0.4f}'])\n",
        "\n",
        "    average_f_score = np.average(paired_f_scores, weights=ks)\n",
        "    print(table)\n",
        "    print(f'=> Average Paired F-Score:  {average_f_score:.4f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw93xw7XNFDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(123)\n",
        "\n",
        "# TASK 3.1\n",
        "def cluster_random(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases randomly\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word] # number of clusters for target word\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be randomly chosen\n",
        "        for cluster in range(k): \n",
        "          # each word must have a cluster, each cluster must have a word\n",
        "          cluster_list = random.choices(paraphrase_list, k = int(np.round(len(paraphrase_list)/k)))\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSsMGhZvNF5J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "3b87afeb-fcbd-44ca-bcb0-5c0e32d845c6"
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    write.v     | 9  |     0.2313     |\n",
            "|    party.n     | 5  |     0.2421     |\n",
            "|    simple.a    | 5  |     0.1333     |\n",
            "|     talk.v     | 6  |     0.3318     |\n",
            "|   operate.v    | 7  |     0.2544     |\n",
            "|    climb.v     | 6  |     0.2069     |\n",
            "|   produce.v    | 7  |     0.2305     |\n",
            "|    degree.n    | 7  |     0.2956     |\n",
            "|    watch.v     | 5  |     0.2393     |\n",
            "|     bank.n     | 9  |     0.2326     |\n",
            "| performance.n  | 5  |     0.3240     |\n",
            "|     mean.v     | 6  |     0.2431     |\n",
            "|   receive.v    | 13 |     0.0603     |\n",
            "|  difference.n  | 5  |     0.3724     |\n",
            "|     eat.v      | 6  |     0.2922     |\n",
            "|    image.n     | 9  |     0.1646     |\n",
            "|     miss.v     | 8  |     0.2316     |\n",
            "|     wash.v     | 13 |     0.1697     |\n",
            "|    treat.v     | 8  |     0.2271     |\n",
            "|   express.v    | 7  |     0.2300     |\n",
            "|     note.v     | 3  |     0.5957     |\n",
            "|   provide.v    | 7  |     0.3317     |\n",
            "|     plan.n     | 3  |     0.4989     |\n",
            "|   interest.n   | 5  |     0.2340     |\n",
            "|     rule.v     | 7  |     0.1839     |\n",
            "|    expect.v    | 6  |     0.2927     |\n",
            "|    begin.v     | 8  |     0.2132     |\n",
            "|     hear.v     | 5  |     0.2740     |\n",
            "|     use.v      | 6  |     0.3747     |\n",
            "|     win.v      | 4  |     0.2885     |\n",
            "|     play.v     | 34 |     0.0543     |\n",
            "| organization.n | 7  |     0.2191     |\n",
            "|   suspend.v    | 6  |     0.2034     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   shelter.n    | 5  |     0.3463     |\n",
            "|    source.n    | 9  |     0.1728     |\n",
            "|    smell.v     | 4  |     0.5169     |\n",
            "|  atmosphere.n  | 6  |     0.2500     |\n",
            "|    paper.n     | 7  |     0.3158     |\n",
            "|   judgment.n   | 7  |     0.1887     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.2261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaenT8V-NJbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_random(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_random.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvKfEy3xNMSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_syllables(word): \n",
        "\n",
        "    word = word.lower() \n",
        "    # exception_add are words that need extra syllables\n",
        "    # exception_del are words that need less syllables\n",
        "    exception_add = ['serious','crucial']\n",
        "    exception_del = ['fortunately','unfortunately'] \n",
        "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
        "    co_two = ['coapt','coed','coinci']\n",
        "    pre_one = ['preach']\n",
        " \n",
        "    syls = 0 # added syllable number\n",
        "    disc = 0 # discarded syllable number\n",
        " \n",
        "    #1) if letters < 3 : return 1\n",
        "    if len(word) <= 3 :\n",
        "        syls = 1\n",
        "        return syls\n",
        " \n",
        "    #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
        "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.) \n",
        "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
        "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
        "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
        "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
        "                pass\n",
        "            else :\n",
        "                disc += 1\n",
        " \n",
        "    #3) discard trailing \"e\", except where ending is \"le\"   \n",
        "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while'] \n",
        "    if word[-1:] == \"e\" :\n",
        "        if word[-2:] == \"le\" and word not in le_except :\n",
        "            pass \n",
        "        else :\n",
        "            disc += 1\n",
        " \n",
        "    #4) check if consecutive vowels exists, triplets or pairs, count them as one. \n",
        "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
        "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
        "    disc += doubleAndtripple + tripple\n",
        " \n",
        "    # 5) count remaining vowels in word.\n",
        "    numVowels = len(re.findall(r'[eaoui]',word))\n",
        " \n",
        "    # 6) add one if starts with \"mc\"\n",
        "    if word[:2] == \"mc\" :\n",
        "        syls+=1\n",
        " \n",
        "    # 7) add one if ends with \"y\" but is not surrouned by vowel\n",
        "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
        "        syls +=1\n",
        " \n",
        "    # 8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
        "    for i,j in enumerate(word) :\n",
        "        if j == \"y\" :\n",
        "            if (i != 0) and (i != len(word)-1) :\n",
        "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
        "                    syls+=1\n",
        " \n",
        "    # 9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one. \n",
        "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
        "        syls+=1 \n",
        "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
        "        syls+=1\n",
        " \n",
        "    # 10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
        "    if word[-3:] == \"ian\" : \n",
        "    # and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
        "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, \n",
        "    #     if not, check if in single dictionary and act accordingly. \n",
        "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
        "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
        "            syls+=1\n",
        "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, \n",
        "    #     if not, check if in single dictionary and act accordingly. \n",
        "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
        "        if word[:6] in pre_one :\n",
        "            pass\n",
        "        else :\n",
        "            syls+=1\n",
        " \n",
        "    # 13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
        "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"] \n",
        "    if word[-3:] == \"n't\" :\n",
        "        if word in negative :\n",
        "            syls+=1\n",
        "        else :\n",
        "            pass  \n",
        " \n",
        "    # 14) Handling the exceptional words. \n",
        "    if word in exception_del :\n",
        "        disc+=1 \n",
        "    if word in exception_add :\n",
        "        syls+=1    \n",
        " \n",
        "    # calculate the output\n",
        "    return numVowels - disc + syls\n",
        "\n",
        "\n",
        "def load_ngram_counts(ngram_counts_file): \n",
        "   counts = defaultdict(int) \n",
        "   with gzip.open(ngram_counts_file, 'rt') as f: \n",
        "       for line in f:\n",
        "           token, count = line.strip().split('\\t') \n",
        "           if token[0].islower(): \n",
        "               counts[token] = int(count) \n",
        "   return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CYo0ihGNRgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(paraphrase):\n",
        "  features_dict = {'pos': nltk.pos_tag(paraphrase)[0][1],\n",
        "                   'syl': count_syllables(paraphrase)}\n",
        "  return features_dict\n",
        "\n",
        "\n",
        "# TASK 3.2\n",
        "def cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'coocvec-500mostfreq-window-3.filter.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = False)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word]\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X_vector = np.zeros((len(paraphrase_list), 500))\n",
        "        # X_features = []\n",
        "        # build data matrix\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          if paraphrase not in vectors:\n",
        "            # print(paraphrase)\n",
        "            X_vector[idx, :] = random.choice(vectors)[1]\n",
        "          X_vector[idx, :] = vectors.query(paraphrase)\n",
        "          # features_dict = extract_features(paraphrase)\n",
        "          # X_features.append(features_dict)\n",
        "        vectorizer = DictVectorizer(sparse = False)\n",
        "\n",
        "        # X_features = vectorizer.fit_transform(X_features)\n",
        "\n",
        "        # X = np.concatenate((X_vector, X_features), axis = 1)\n",
        "        X = X_vector\n",
        "\n",
        "        # kmeans = KMeans(n_clusters = k).fit(X)\n",
        "        AC = AgglomerativeClustering(n_clusters = k, affinity = 'manhattan', linkage = 'single').fit(X)\n",
        "\n",
        "        for cluster in range(k):\n",
        "          # cluster_list = [paraphrase_list[idx] \n",
        "          #                 for idx, label in enumerate(kmeans.labels_) \n",
        "          #                 if label == cluster]\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(AC.labels_) \n",
        "                          if label == cluster]\n",
        "          if len(cluster_list) == 0:\n",
        "            print('Empty cluster\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9zdRBjvNXNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "99da09e6-7938-4ce4-8f47-4ae1123c92a7"
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    write.v     | 9  |     0.3669     |\n",
            "|    party.n     | 5  |     0.3882     |\n",
            "|    simple.a    | 5  |     0.3590     |\n",
            "|     talk.v     | 6  |     0.6338     |\n",
            "|   operate.v    | 7  |     0.2844     |\n",
            "|    climb.v     | 6  |     0.3375     |\n",
            "|   produce.v    | 7  |     0.4698     |\n",
            "|    degree.n    | 7  |     0.4182     |\n",
            "|    watch.v     | 5  |     0.3300     |\n",
            "|     bank.n     | 9  |     0.3000     |\n",
            "| performance.n  | 5  |     0.4840     |\n",
            "|     mean.v     | 6  |     0.3714     |\n",
            "|   receive.v    | 13 |     0.1357     |\n",
            "|  difference.n  | 5  |     0.5337     |\n",
            "|     eat.v      | 6  |     0.4341     |\n",
            "|    image.n     | 9  |     0.3332     |\n",
            "|     miss.v     | 8  |     0.3333     |\n",
            "|     wash.v     | 13 |     0.3602     |\n",
            "|    treat.v     | 8  |     0.4077     |\n",
            "|   express.v    | 7  |     0.4187     |\n",
            "|     note.v     | 3  |     0.6400     |\n",
            "|   provide.v    | 7  |     0.7374     |\n",
            "|     plan.n     | 3  |     0.6387     |\n",
            "|   interest.n   | 5  |     0.3121     |\n",
            "|     rule.v     | 7  |     0.3481     |\n",
            "|    expect.v    | 6  |     0.4924     |\n",
            "|    begin.v     | 8  |     0.2892     |\n",
            "|     hear.v     | 5  |     0.3319     |\n",
            "|     use.v      | 6  |     0.6461     |\n",
            "|     win.v      | 4  |     0.5247     |\n",
            "|     play.v     | 34 |     0.1904     |\n",
            "| organization.n | 7  |     0.3917     |\n",
            "|   suspend.v    | 6  |     0.2453     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   shelter.n    | 5  |     0.5032     |\n",
            "|    source.n    | 9  |     0.2978     |\n",
            "|    smell.v     | 4  |     0.5484     |\n",
            "|  atmosphere.n  | 6  |     0.4203     |\n",
            "|    paper.n     | 7  |     0.6747     |\n",
            "|   judgment.n   | 7  |     0.3296     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLA5RWX7Naxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_with_sparse_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_sparse.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLrluVfJNfd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 3.3\n",
        "def cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using sparse vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :param word_to_k_dict: dictionary, where key is a target word and value is a number of clusters\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'GoogleNews-vectors-negative300.filter.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = True)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = word_to_k_dict[target_word]\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X_vector = np.zeros((len(paraphrase_list), 300))\n",
        "        # X_features = []\n",
        "        # build data matrix\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          # if paraphrase not in vectors:\n",
        "          #   # print(paraphrase)\n",
        "          #   X_vector[idx, :] = random.choice(vectors)[1]\n",
        "          X_vector[idx, :] = vectors.query(paraphrase)\n",
        "          # features_dict = extract_features(paraphrase)\n",
        "          # X_features.append(features_dict)\n",
        "        vectorizer = DictVectorizer(sparse = False)\n",
        "\n",
        "        # X_features = vectorizer.fit_transform(X_features)\n",
        "\n",
        "        # X = np.concatenate((X_vector, X_features), axis = 1)\n",
        "        X = X_vector\n",
        "\n",
        "        # kmeans = KMeans(n_clusters = k).fit(X)\n",
        "        AC = AgglomerativeClustering(n_clusters = k, affinity = 'manhattan', linkage = 'single').fit(X)\n",
        "\n",
        "        for cluster in range(k):\n",
        "          # cluster_list = [paraphrase_list[idx] \n",
        "          #                 for idx, label in enumerate(kmeans.labels_) \n",
        "          #                 if label == cluster]\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(AC.labels_) \n",
        "                          if label == cluster]\n",
        "          if len(cluster_list) == 0:\n",
        "            print('Empty cluster\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqHf9PPyNi3K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "82108083-962f-42a5-8a46-3138604c36dc"
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+----+----------------+\n",
            "|     Target     | k  | Paired F-Score |\n",
            "+----------------+----+----------------+\n",
            "|    write.v     | 9  |     0.3400     |\n",
            "|    party.n     | 5  |     0.4323     |\n",
            "|    simple.a    | 5  |     0.3448     |\n",
            "|     talk.v     | 6  |     0.6504     |\n",
            "|   operate.v    | 7  |     0.3124     |\n",
            "|    climb.v     | 6  |     0.3176     |\n",
            "|   produce.v    | 7  |     0.4548     |\n",
            "|    degree.n    | 7  |     0.4315     |\n",
            "|    watch.v     | 5  |     0.5300     |\n",
            "|     bank.n     | 9  |     0.6154     |\n",
            "| performance.n  | 5  |     0.4718     |\n",
            "|     mean.v     | 6  |     0.3646     |\n",
            "|   receive.v    | 13 |     0.2141     |\n",
            "|  difference.n  | 5  |     0.5107     |\n",
            "|     eat.v      | 6  |     0.4614     |\n",
            "|    image.n     | 9  |     0.3278     |\n",
            "|     miss.v     | 8  |     0.3964     |\n",
            "|     wash.v     | 13 |     0.3403     |\n",
            "|    treat.v     | 8  |     0.3903     |\n",
            "|   express.v    | 7  |     0.4206     |\n",
            "|     note.v     | 3  |     0.8400     |\n",
            "|   provide.v    | 7  |     0.7139     |\n",
            "|     plan.n     | 3  |     0.7261     |\n",
            "|   interest.n   | 5  |     0.3716     |\n",
            "|     rule.v     | 7  |     0.3165     |\n",
            "|    expect.v    | 6  |     0.5079     |\n",
            "|    begin.v     | 8  |     0.3971     |\n",
            "|     hear.v     | 5  |     0.3789     |\n",
            "|     use.v      | 6  |     0.6316     |\n",
            "|     win.v      | 4  |     0.4892     |\n",
            "|     play.v     | 34 |     0.1566     |\n",
            "| organization.n | 7  |     0.3904     |\n",
            "|   suspend.v    | 6  |     0.3256     |\n",
            "|  different.a   | 1  |     1.0000     |\n",
            "|   shelter.n    | 5  |     0.5821     |\n",
            "|    source.n    | 9  |     0.3534     |\n",
            "|    smell.v     | 4  |     0.4677     |\n",
            "|  atmosphere.n  | 6  |     0.4709     |\n",
            "|    paper.n     | 7  |     0.5633     |\n",
            "|   judgment.n   | 7  |     0.3336     |\n",
            "+----------------+----+----------------+\n",
            "=> Average Paired F-Score:  0.3997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqY0q0_9Nl8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, word_to_k_dict = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_input.txt')\n",
        "# predicted_clusterings = cluster_with_dense_representation(word_to_paraphrases_dict, word_to_k_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_dense.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp9vaidDNn-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TASK 3.4\n",
        "\n",
        "def choose_k(paraphrase_list, vectors):\n",
        "\n",
        "  silhouette_scores = []\n",
        "  for k in range(2, min(20, len(paraphrase_list))):\n",
        "    X_vector = np.zeros((len(paraphrase_list), 300))\n",
        "    for idx, paraphrase in enumerate(paraphrase_list):\n",
        "      X_vector[idx, :] = vectors.query(paraphrase)\n",
        "    X = X_vector\n",
        "    AC = AgglomerativeClustering(n_clusters = k, affinity = 'manhattan', linkage = 'single').fit(X)\n",
        "    cluster_labels = AC.fit_predict(X)\n",
        "    silhouette_average = silhouette_score(X, cluster_labels, metric = 'manhattan')\n",
        "    silhouette_scores.append(silhouette_average)\n",
        "  if len(silhouette_scores) == 0:\n",
        "    # k_choice = random.randint(1,2) # randomly choose between either 1 or 2 clusters\n",
        "    k_choice = 1\n",
        "  else:\n",
        "    k_choice = range(2, min(20, len(paraphrase_list)))[np.argmax(silhouette_scores)]\n",
        "  return k_choice\n",
        "\n",
        "def cluster_with_no_k(word_to_paraphrases_dict):\n",
        "    \"\"\"\n",
        "    Clusters paraphrases using any vector representation\n",
        "    :param word_to_paraphrases_dict: dictionary, where key is a target word and value is a list of paraphrases\n",
        "    :return: dictionary, where key is a target word and value is a list of list of paraphrases,\n",
        "    where each list corresponds to a cluster\n",
        "    \"\"\"\n",
        "    # Note: any vector representation should be in the same directory as this file\n",
        "    vectors_root_path = 'drive/My Drive/CIS-530/Homework 5/Data/vectors/'\n",
        "    vectors_path = 'GoogleNews-vectors-negative300.magnitude'\n",
        "    # vectors_path = 'glove.840B.300d.magnitude'\n",
        "    vectors = Magnitude(vectors_root_path + vectors_path, normalized = True)\n",
        "\n",
        "    clusterings = {}\n",
        "\n",
        "    for target_word in word_to_paraphrases_dict.keys():\n",
        "        paraphrase_list = word_to_paraphrases_dict[target_word]\n",
        "        clusters = []\n",
        "        k = choose_k(paraphrase_list, vectors)\n",
        "        chosen_paraphrases = set() # keep track of any paraphrases that may not be chosen\n",
        "        X_vector = np.zeros((len(paraphrase_list), 300))\n",
        "        for idx, paraphrase in enumerate(paraphrase_list):\n",
        "          X_vector[idx, :] = vectors.query(paraphrase)\n",
        "\n",
        "        vectorizer = DictVectorizer(sparse = False)\n",
        "\n",
        "        X = X_vector\n",
        "\n",
        "        AC = AgglomerativeClustering(n_clusters = k, affinity = 'manhattan', linkage = 'single').fit(X)\n",
        "\n",
        "        for cluster in range(k):\n",
        "          cluster_list = [paraphrase_list[idx] \n",
        "                          for idx, label in enumerate(AC.labels_) \n",
        "                          if label == cluster]\n",
        "          if len(cluster_list) == 0:\n",
        "            print('Empty cluster.\\n')\n",
        "          chosen_paraphrases.update(cluster_list)\n",
        "          clusters.append(cluster_list)\n",
        "        for paraphrase in paraphrase_list:\n",
        "          if paraphrase not in chosen_paraphrases:\n",
        "            # choose a random cluster list and append unassigned word to it\n",
        "            random.choice(clusters).append(paraphrase) \n",
        "        clusterings[target_word] = clusters\n",
        "\n",
        "    return clusterings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qxpVwzsNtIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "030a7758-62ce-4265-9b99-39147deaf904"
      },
      "source": [
        "input_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_input.txt'\n",
        "output_filepath = 'drive/My Drive/CIS-530/Homework 5/Data/data/dev_output.txt'\n",
        "word_to_paraphrases_dict, word_to_k_dict = load_input_file(input_filepath)\n",
        "gold_clusterings = load_output_file(output_filepath)\n",
        "predicted_clusterings = cluster_with_no_k(word_to_paraphrases_dict)\n",
        "evaluate_clusterings(gold_clusterings, predicted_clusterings)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DatabaseError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-dc848529393d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword_to_paraphrases_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_k_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_input_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgold_clusterings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_output_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpredicted_clusterings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_with_no_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_paraphrases_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mevaluate_clusterings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_clusterings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_clusterings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bbd95ee25ec9>\u001b[0m in \u001b[0;36mcluster_with_no_k\u001b[0;34m(word_to_paraphrases_dict)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mparaphrase_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_to_paraphrases_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparaphrase_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mchosen_paraphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# keep track of any paraphrases that may not be chosen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mX_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparaphrase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-bbd95ee25ec9>\u001b[0m in \u001b[0;36mchoose_k\u001b[0;34m(paraphrase_list, vectors)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparaphrase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparaphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparaphrase_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mX_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparaphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mAC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgglomerativeClustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maffinity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'manhattan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinkage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'single'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/repoze/lru/__init__.py\u001b[0m in \u001b[0;36mcached_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                     \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, q, pad_to_length, pad_left, truncate_left, normalized)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vector_for_key_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_of_vocab_vector_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymagnitude/third_party/repoze/lru/__init__.py\u001b[0m in \u001b[0;36mcached_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                     \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36m_out_of_vocab_vector_cached\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 remove_self=True)\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0m_out_of_vocab_vector_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_of_vocab_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key_for_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36m_out_of_vocab_vector\u001b[0;34m(self, key, normalized, force)\u001b[0m\n\u001b[1;32m    990\u001b[0m                     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                     \u001b[0morig_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                     normalized=normalized) *\n\u001b[0m\u001b[1;32m    993\u001b[0m                 0.7)\n\u001b[1;32m    994\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36m_db_query_similar_keys_vector\u001b[0;34m(self, key, orig_key, topn, normalized)\u001b[0m\n\u001b[1;32m    827\u001b[0m                                           for n in ngrams), topn)\n\u001b[1;32m    828\u001b[0m                     results = self._db().execute(search_query,\n\u001b[0;32m--> 829\u001b[0;31m                                                  params).fetchall()\n\u001b[0m\u001b[1;32m    830\u001b[0m                     small_typo = len(results) > 0 and self._string_dist(\n\u001b[1;32m    831\u001b[0m                         results[0][0].lower(), orig_key.lower()) <= 4\n",
            "\u001b[0;31mDatabaseError\u001b[0m: database disk image is malformed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTPncp-sNylX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word_to_paraphrases_dict, _ = load_input_file('drive/My Drive/CIS-530/Homework 5/Data/data/test_nok_input.txt')\n",
        "# predicted_clusterings = cluster_with_no_k(word_to_paraphrases_dict)\n",
        "# write_to_output_file('drive/My Drive/CIS-530/Homework 5/test_output_nok.txt', predicted_clusterings)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}