{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "def read_in_shakespeare():\n",
    "    '''Reads in the Shakespeare dataset processes it into a list of tuples.\n",
    "    Also reads in the vocab and play name lists from files.\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "    \n",
    "    Returns:\n",
    "    tuples: A list of tuples in the above format.\n",
    "    document_names: A list of the plays present in the corpus.\n",
    "    vocab: A list of all tokens in the vocabulary.\n",
    "    '''\n",
    "    tuples = []\n",
    "    with open('Data/will_play_text.csv') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "            tuples.append((play_name, line_tokens))\n",
    "            \n",
    "    with open('Data/vocab.txt') as f:\n",
    "        vocab =  [line.strip() for line in f]\n",
    "    \n",
    "    with open('Data/play_names.txt') as f:\n",
    "        document_names =  [line.strip() for line in f]\n",
    "        \n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "def get_row_vector(matrix, row_id):\n",
    "    return matrix[row_id, :]\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "    return matrix[:, col_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "    Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "    a tokenized line from that document.\n",
    "    document_names: A list of the document names\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "    \n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "    \n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "    \n",
    "    Returns:\n",
    "    td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "                and each column corresponds to a document. A_ij contains the\n",
    "                frequency with which word i occurs in document j.\n",
    "    '''\n",
    "    vocab_to_ID = dict(zip(vocab, range(0, len(vocab))))\n",
    "    docname_to_ID = dict(zip(document_names, range(0, len(document_names))))\n",
    "    \n",
    "    m = len(vocab)\n",
    "    n = len(document_names)\n",
    "    term_doc_mat = np.zeros([m, n])\n",
    "    \n",
    "    for line_tuple in line_tuples:\n",
    "        doc_name = line_tuple[0]\n",
    "        doc_ID = docname_to_ID[doc_name] # get ID corresponding to document\n",
    "        line = line_tuple[1]\n",
    "        for word in line:\n",
    "            word_ID = vocab_to_ID[word] # get ID corresponding to word\n",
    "            term_doc_mat[word_ID][doc_ID] += 1 # increment word count\n",
    "    \n",
    "    return term_doc_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_context_matrix(line_tuples, vocab, context_window_size = 1):\n",
    "    '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "    Inputs:\n",
    "    line_tuples: A list of tuples, containing the name of the document and \n",
    "                    a tokenized line from that document.\n",
    "    vocab: A list of the tokens in the vocabulary\n",
    "    \n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:39 PM.\n",
    "    \n",
    "    Let n = len(vocab).\n",
    "    \n",
    "    Returns:\n",
    "    tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "                word j was found within context_window_size to the left or right of\n",
    "                word i in any sentence in the tuples.\n",
    "  '''\n",
    "    vocab_to_ID = dict(zip(vocab, range(0, len(vocab))))\n",
    "    \n",
    "    m = len(vocab)\n",
    "    term_term_mat = np.zeros([m, m])\n",
    "\n",
    "    for line_tuple in line_tuples:\n",
    "        doc_ID = line_tuple[0]\n",
    "        line = line_tuple[1]\n",
    "        for word_idx, word in enumerate(line):\n",
    "            word_ID = vocab_to_ID[word] # target word\n",
    "            for context_word_idx in range(1, context_window_size + 1):\n",
    "                # look behind\n",
    "                prev_word_idx = word_idx - context_word_idx\n",
    "                if prev_word_idx >= 0:\n",
    "                    prev_word = line[prev_word_idx]\n",
    "                    prev_word_ID = vocab_to_ID[prev_word]\n",
    "                    term_term_mat[word_ID, prev_word_ID] += 1\n",
    "                # look ahead\n",
    "                next_word_idx = word_idx + context_word_idx\n",
    "                if next_word_idx < len(line):\n",
    "                    next_word = line[next_word_idx]\n",
    "                    next_word_ID = vocab_to_ID[next_word]\n",
    "                    term_term_mat[word_ID, next_word_ID] += 1\n",
    "         \n",
    "    return term_term_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_tuples, document_names, vocab = read_in_shakespeare()\n",
    "# term_term_matrix = create_term_context_matrix(line_tuples, vocab, context_window_size = 2)\n",
    "# term_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PPMI_matrix(term_context_matrix):\n",
    "    '''Given a term context matrix, output a PPMI matrix.\n",
    "    See section 15.1 in the textbook.\n",
    "    \n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "    \n",
    "    Input:\n",
    "    term_context_matrix: A nxn numpy array, where n is\n",
    "                            the numer of tokens in the vocab.\n",
    "                            \n",
    "    Returns: A nxn numpy matrix, where A_ij is equal to the\n",
    "                point-wise mutual information between the ith word\n",
    "                and the jth word in the term_context_matrix.\n",
    "  '''       \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "\n",
    "def create_tf_idf_matrix(term_document_matrix):\n",
    "    '''Given the term document matrix, output a tf-idf weighted version.\n",
    "    \n",
    "    See section 15.2.1 in the textbook.\n",
    "    \n",
    "    Hint: Use numpy matrix and vector operations to speed up implementation.\n",
    "    \n",
    "    Input:\n",
    "    term_document_matrix: Numpy array where each column represents a document \n",
    "                            and each row, the frequency of a word in that document.\n",
    "    \n",
    "    Returns:\n",
    "    A numpy array with the same dimension as term_document_matrix, where\n",
    "    A_ij is weighted by the inverse document frequency of document h.\n",
    "  '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "\n",
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "    \n",
    "    Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "    \n",
    "    Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return -1\n",
    "\n",
    "def compute_jaccard_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "    Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "    \n",
    "    Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "    # YOUR CODE HERE\n",
    "    return -1\n",
    "\n",
    "def compute_dice_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "    Inputs:\n",
    "    vector1: A nx1 numpy array\n",
    "    vector2: A nx1 numpy array\n",
    "    \n",
    "    Returns:\n",
    "    A scalar similarity value.\n",
    "  '''\n",
    "    # YOUR CODE HERE\n",
    "    return -1\n",
    "\n",
    "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the plays to the target play.\n",
    "    \n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "    \n",
    "    Inputs:\n",
    "    target_play_index: The integer index of the play we want to compare all others against.\n",
    "    term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "    similarity_fn: Function that should be used to compared vectors for two\n",
    "                    documents. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "                    compute_cosine_similarity.\n",
    "                    \n",
    "    Returns:\n",
    "    A length-n list of integer indices corresponding to play names,\n",
    "    ordered by decreasing similarity to the play indexed by target_play_index\n",
    "  '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    return []\n",
    "\n",
    "def rank_words(target_word_index, matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the words to the target word.\n",
    "    # NOTE: THIS DOCSTRING WAS UPDATED ON JAN 24, 12:51 PM.\n",
    "    Inputs:\n",
    "    target_word_index: The index of the word we want to compare all others against.\n",
    "    matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "    similarity_fn: Function that should be used to compared vectors for two word\n",
    "                    embeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "                    compute_cosine_similarity.\n",
    "    \n",
    "    Returns:\n",
    "    A length-n list of integer word indices, ordered by decreasing similarity to the \n",
    "    target word indexed by word_index\n",
    "  '''\n",
    "    # YOUR CODE HERE\n",
    "    return []\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tuples, document_names, vocab = read_in_shakespeare()\n",
    "    \n",
    "    print('Computing term document matrix...')\n",
    "    td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "    \n",
    "    print('Computing tf-idf matrix...')\n",
    "    tf_idf_matrix = create_tf_idf_matrix(td_matrix)\n",
    "\n",
    "    print('Computing term context matrix...')\n",
    "    tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "    \n",
    "    print('Computing PPMI matrix...')\n",
    "    PPMI_matrix = create_PPMI_matrix(tc_matrix)\n",
    "\n",
    "    random_idx = random.randint(0, len(document_names)-1)\n",
    "    similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar plays to \"%s\" using %s are:' % (document_names[random_idx], sim_fn.__qualname__))\n",
    "        ranks = rank_plays(random_idx, td_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            doc_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, document_names[doc_id]))\n",
    "            \n",
    "    word = 'juliet'\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar words to \"%s\" using %s on term-context frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "        ranks = rank_words(vocab_to_index[word], tc_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            word_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, vocab[word_id]))\n",
    "\n",
    "    word = 'juliet'\n",
    "    vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "    for sim_fn in similarity_fns:\n",
    "        print('\\nThe 10 most similar words to \"%s\" using %s on PPMI matrix are:' % (word, sim_fn.__qualname__))\n",
    "        ranks = rank_words(vocab_to_index[word], PPMI_matrix, sim_fn)\n",
    "        for idx in range(0, 10):\n",
    "            word_id = ranks[idx]\n",
    "            print('%d: %s' % (idx+1, vocab[word_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:everything] *",
   "language": "python",
   "name": "conda-env-everything-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
